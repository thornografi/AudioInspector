// Side panel script
let latestData = null;
let enabled = false; // Default to false (stopped)
let drawerOpen = false; // Console drawer state
let currentTabId = null; // Track which tab this panel is associated with
let updateUITimer = null; // Debounce timer for updateUI() calls

// Constants - MUST be kept in sync with src/core/constants.js
// (popup.js cannot import ES modules, so values are duplicated here)
const DESTINATION_TYPES = {
  SPEAKERS: 'speakers',
  MEDIA_STREAM: 'MediaStreamDestination'
};
const MAX_AUDIO_CONTEXTS = 4; // UI_LIMITS.MAX_AUDIO_CONTEXTS in constants.js

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// DRY: Storage keys fetched from background.js (single source of truth)
// Fallback array used until background.js responds (prevents race condition)
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
let DATA_STORAGE_KEYS = ['rtc_stats', 'user_media', 'audio_contexts', 'audio_worklet', 'media_recorder', 'detected_encoder', 'audio_connections', 'recording_active'];

// Fetch actual keys from background.js (async, updates DATA_STORAGE_KEYS)
chrome.runtime.sendMessage({ type: 'GET_STORAGE_KEYS' }, (response) => {
  if (response?.keys) {
    DATA_STORAGE_KEYS = response.keys;
  }
});

/**
 * Centralized cleanup request helper
 * DRY: Delegates to background.js (single source of truth)
 * @param {string} preset - One of background.js CLEANUP_PRESETS keys
 * @param {Object} [options] - Optional overrides (rare)
 * @returns {Promise<void>}
 */
function requestCleanup(preset, options) {
  return new Promise((resolve) => {
    chrome.runtime.sendMessage({ type: 'CLEAR_INSPECTOR_DATA', preset, options }, () => resolve());
  });
}

/**
 * Clear inspector state + data + logs (full cleanup)
 * @param {Object} [options]
 * @returns {Promise<void>}
 */
function clearInspectorData(options) {
  return requestCleanup('FULL', options);
}

/**
 * Clear measurement data + logs, keep state (session cleanup)
 * @param {Object} [options]
 * @returns {Promise<void>}
 */
function clearSessionData(options) {
  return requestCleanup('SESSION', options);
}

/**
 * Clear only debug logs
 * @returns {Promise<void>}
 */
function clearLogsOnly() {
  return requestCleanup('LOGS_ONLY');
}

/**
 * Check if a URL is a system page (chrome://, about://, etc.)
 * System pages don't support content script injection
 * DRY helper - used in hideLockedTabInfo() and updateControlsForCurrentTab()
 * @param {string|undefined} url - URL to check
 * @returns {boolean} true if system page or no URL
 */
function isSystemPage(url) {
  if (!url) return true;
  return url.startsWith('chrome://') ||
         url.startsWith('chrome-extension://') ||
         url.startsWith('about:') ||
         url.startsWith('file://');
}

// Debug log helper - background.js Ã¼zerinden merkezi yÃ¶netim (race condition Ã¶nleme)
function debugLog(message) {
  const entry = {
    timestamp: Date.now(),
    level: 'info',
    prefix: 'Popup',
    message: message
  };

  chrome.runtime.sendMessage({ type: 'ADD_LOG', entry: entry });
}

// Debounced UI update wrapper
// Batches rapid storage changes (e.g., audio_contexts + audio_connections) into single UI update
function updateUIDebounced() {
  if (updateUITimer !== null) {
    clearTimeout(updateUITimer);
  }

  updateUITimer = setTimeout(() => {
    updateUI();
    updateUITimer = null;
  }, 50); // 50ms debounce - waits for both context (20ms) + connections (16ms) debounces
}

// Main update function
async function updateUI() {
  // Get all relevant data from storage in one go
  const result = await chrome.storage.local.get([
    'rtc_stats',
    'user_media',
    'audio_contexts',
    'audio_connections',
    'detected_encoder',
    'media_recorder',
    'recording_active',
    'debug_logs',
    'lastUpdate',
    'lockedTab'
  ]);

  latestData = result; // Keep a copy for export

  // Tab ID validation - only show data from current locked tab
  // CRITICAL: When no tab is locked, NO data should be shown (prevents stale data display)
  const lockedTabId = result.lockedTab?.id;
  const isValidData = (data) => !data || (lockedTabId && data.sourceTabId === lockedTabId);

  // Filter audio_contexts array by sourceTabId
  // Only show contexts from the locked tab, nothing if no tab is locked
  const validAudioContexts = result.audio_contexts?.filter(ctx =>
    lockedTabId && ctx.sourceTabId === lockedTabId
  );

  // Filter audio_connections by sourceTabId
  const validAudioConnections = isValidData(result.audio_connections)
    ? result.audio_connections
    : null;

  // CANONICAL: Read from detected_encoder storage key (unified encoder detection)
  // Both URL pattern detection and opus hook detection emit to this key
  const validDetectedEncoder = isValidData(result.detected_encoder) ? result.detected_encoder : null;

  // Render each section with validated data
  // Data from different tabs is filtered out to prevent stale data display
  const validRtcStats = isValidData(result.rtc_stats) ? result.rtc_stats : null;
  const validMediaRecorder = isValidData(result.media_recorder) ? result.media_recorder : null;

  const validUserMedia = isValidData(result.user_media) ? result.user_media : null;
  const validRecordingActive = isValidData(result.recording_active) ? result.recording_active : null;

  renderRTCStats(validRtcStats);
  renderGUMStats(validUserMedia);
  renderACStats(validAudioContexts?.length > 0 ? validAudioContexts : null, validAudioConnections);
  renderEncodingSection(validDetectedEncoder, validRtcStats, validMediaRecorder, validAudioContexts?.length > 0 ? validAudioContexts : null, validUserMedia, validRecordingActive);
  renderDebugLogs(result.debug_logs);
}

// Load inspector enabled state from storage
async function loadEnabledState() {
  const result = await chrome.storage.local.get('inspectorEnabled');
  // Default to false (stopped) - user must explicitly enable
  enabled = result.inspectorEnabled === true;
  updateToggleButton();
}

// Toggle inspector on/off
async function toggleInspector() {
  // Get active tab in current window
  const tabs = await chrome.tabs.query({active: true, currentWindow: true, url: ["http://*/*", "https://*/*"]});
  const activeTab = tabs[0];

  // STOP durumunda kilitli tab bilgisini al (mesajÄ± oraya gÃ¶ndermek iÃ§in)
  const result = await chrome.storage.local.get(['lockedTab', 'inspectorEnabled']);
  const lockedTab = result.lockedTab;

  // START durumunda geÃ§erli tab yoksa iÅŸlem yapma (chrome://, about:, vb.)
  if (!enabled && !activeTab) {
    debugLog('âš ï¸ No valid tab to start inspector (chrome:// or about:// pages are not supported)');
    return;
  }

  // CRITICAL: Second start on same tab requires page refresh
  // Detects: inspector stopped + lockedTab exists + same tab = previous session data exists
  if (!enabled && lockedTab && lockedTab.id === activeTab?.id) {
    debugLog('âš ï¸ Second start detected on same tab - showing refresh modal');
    showRefreshModal(activeTab);
    return; // Don't proceed - wait for user decision
  }

  // Åžimdi gÃ¼venle toggle yap
  enabled = !enabled;

  if (enabled && activeTab) {
    // START: Clear any stale auto-stop reason first (prevents 3-4s banner flash)
    await chrome.storage.local.remove(['autoStoppedReason']);

    // START: Aktif tab'Ä± kilitle
    const lockedTabData = {
      id: activeTab.id,
      url: activeTab.url,
      title: activeTab.title
    };
    debugLog(`ðŸ”’ Tab kilitlendi: ${activeTab.url} (id: ${activeTab.id})`);
    await chrome.storage.local.set({
      inspectorEnabled: true,
      lockedTab: lockedTabData
    });

    // MesajÄ± aktif tab'a gÃ¶nder
    chrome.tabs.sendMessage(activeTab.id, {
      type: 'SET_ENABLED',
      enabled: true
    }, () => chrome.runtime.lastError); // Suppress error
  } else {
    // STOP: MesajÄ± KÄ°LÄ°TLÄ° TAB'A gÃ¶nder (farklÄ± tab'dan Stop'a basÄ±lmÄ±ÅŸ olabilir)
    debugLog('ðŸ”“ Tab kilidi kaldÄ±rÄ±ldÄ±');

    if (lockedTab?.id) {
      debugLog(`Stopping inspector on locked tab: ${lockedTab.id}`);
      chrome.tabs.sendMessage(lockedTab.id, {
        type: 'SET_ENABLED',
        enabled: false
      }, () => chrome.runtime.lastError); // Suppress error
    }

    // Stop'ta sadece inspectorEnabled'i sil, lockedTab'i BIRAK
    // Boylece veriler hala o tab'a ait olarak validate edilir ve gosterilir (review icin)
    // lockedTab, tab kapaninca background.js tarafindan temizlenir
    await chrome.storage.local.remove(['inspectorEnabled']);
  }

  // NOTE: Session cleanup (data + logs) is handled by content.js before collectors start emitting.

  // Update button AND UI to reflect new state
  // Note: checkTabLock() is called by storage listener when lockedTab changes (line 933)
  updateToggleButton();
  await updateUI(); // Critical: update label and data display
}

// Update toggle button appearance and recording mode
function updateToggleButton() {
  const btn = document.getElementById('toggleBtn');
  const statusText = document.getElementById('statusText');
  const body = document.body;

  if (enabled) {
    // Inspector Ã§alÄ±ÅŸÄ±yor â†’ Stop butonu gÃ¶ster
    btn.innerHTML = '<span>Stop</span>';
    statusText.textContent = 'Inspecting';
    body.classList.add('inspecting');
  } else {
    // Inspector durmuÅŸ â†’ Start butonu gÃ¶ster
    btn.innerHTML = '<span>Start</span>';
    statusText.textContent = 'Stopped';
    body.classList.remove('inspecting');
  }

  // Update Export/Clear buttons: disabled when inspector is running
  updateActionButtons(enabled);

  // Note: Icon is automatically updated by background.js storage listener
}

// Tab kilitleme kontrolÃ¼ - popup aÃ§Ä±ldÄ±ÄŸÄ±nda Ã§aÄŸrÄ±lÄ±r
async function checkTabLock() {
  const [currentTab] = await chrome.tabs.query({ active: true, currentWindow: true });
  const result = await chrome.storage.local.get(['lockedTab', 'inspectorEnabled', 'autoStoppedReason']);

  debugLog(`checkTabLock: currentTab=${currentTab?.id}, lockedTab=${result.lockedTab?.id}, enabled=${result.inspectorEnabled}`);

  // Auto-stop bildirimi varsa gÃ¶ster ve temizle
  if (result.autoStoppedReason) {
    showAutoStopBanner(result.autoStoppedReason);
    chrome.storage.local.remove(['autoStoppedReason']);
  }

  // lockedTab varsa her zaman banner gÃ¶ster (running veya stopped - review iÃ§in)
  if (result.lockedTab) {
    // Kilitli tab hala var mÄ± kontrol et (edge case: background.js cleanup Ã§alÄ±ÅŸmamÄ±ÅŸ olabilir)
    try {
      await chrome.tabs.get(result.lockedTab.id);
    } catch (e) {
      // Tab artÄ±k yok - kilidi kaldÄ±r ve temizlik yap
      debugLog(`ðŸ§¹ Kilitli tab artÄ±k yok (id: ${result.lockedTab.id}), temizleniyor`);
      await clearInspectorData();
      await hideLockedTabInfo();
      return true;
    }

    const isSameTab = result.lockedTab.id === currentTab?.id;

    // Banner'Ä± gÃ¶ster - inspector Ã§alÄ±ÅŸÄ±yor olsun ya da olmasÄ±n
    showLockedTabInfo(result.lockedTab, isSameTab, result.inspectorEnabled);
    debugLog(`Banner gÃ¶steriliyor (${isSameTab ? 'aynÄ± tab' : 'farklÄ± tab'}, ${result.inspectorEnabled ? 'running' : 'stopped'}): ${result.lockedTab.url}`);

    return isSameTab;
  } else {
    debugLog('lockedTab yok - banner gizleniyor');
    await hideLockedTabInfo();
    return true;
  }
}

// Extract domain from locked tab - SRP: single responsibility for domain extraction
function extractDomain(lockedTab) {
  try {
    return new URL(lockedTab.url).hostname;
  } catch {
    return lockedTab.title || 'Bilinmeyen';
  }
}

// Get banner status text - SRP: single responsibility for text determination
function getBannerStatusText(isSameTab, isRunning) {
  if (isSameTab) {
    return isRunning ? 'Inspecting' : 'Stopped - Data from';
  }
  return 'Different tab - data from';
}

// Update banner CSS classes - SRP: single responsibility for styling
function updateBannerStyle(banner, isSameTab) {
  if (isSameTab) {
    banner.classList.add('visible', 'same-tab');
    banner.classList.remove('different-tab');
  } else {
    banner.classList.add('visible', 'different-tab');
    banner.classList.remove('same-tab');
  }
}

// Kilitli tab bilgisini gÃ¶ster - SRP: orchestration only
function showLockedTabInfo(lockedTab, isSameTab = false, isRunning = false) {
  const banner = document.getElementById('lockedTabBanner');
  const domainSpan = document.getElementById('lockedTabDomain');
  const bannerStatusText = document.getElementById('bannerStatusText');
  const controls = document.querySelector('.controls');

  if (!banner || !domainSpan || !bannerStatusText) {
    debugLog('âŒ showLockedTabInfo: DOM element bulunamadÄ±!');
    return;
  }

  // Extract and set domain
  const domain = extractDomain(lockedTab);
  domainSpan.textContent = domain;

  // Set status text
  bannerStatusText.textContent = getBannerStatusText(isSameTab, isRunning);

  // Update styling
  updateBannerStyle(banner, isSameTab);

  // Controls disabled state: farklÄ± tab'da Start butonu disabled olmalÄ±
  if (isSameTab) {
    controls?.classList.remove('disabled');
  } else {
    controls?.classList.add('disabled');
  }

  debugLog(`âœ… Banner gÃ¶sterildi: ${domain} (${isSameTab ? 'aynÄ±' : 'farklÄ±'} tab, ${isRunning ? 'running' : 'stopped'})`);
}

// Kilitli tab bilgisini gizle
async function hideLockedTabInfo() {
  const banner = document.getElementById('lockedTabBanner');
  const controls = document.querySelector('.controls');

  banner?.classList.remove('visible', 'same-tab', 'different-tab');

  // Controls'u hemen enabled yap, sonra sistem sayfasÄ± kontrolÃ¼ yap
  // Bu async updateControlsForCurrentTab()'dan daha gÃ¼venilir
  const [currentTab] = await chrome.tabs.query({ active: true, currentWindow: true });

  if (isSystemPage(currentTab?.url)) {
    controls?.classList.add('disabled');
  } else {
    controls?.classList.remove('disabled');
  }
}

// Update controls disabled state based on page type (chrome://, about://, etc.)
async function updateControlsForCurrentTab() {
  const [currentTab] = await chrome.tabs.query({ active: true, currentWindow: true });
  const controls = document.querySelector('.controls');

  if (isSystemPage(currentTab?.url)) {
    controls?.classList.add('disabled');
    debugLog(`âš ï¸ System page detected - controls disabled: ${currentTab?.url || 'no URL'}`);
  } else {
    controls?.classList.remove('disabled');
  }
}

// Update Export/Clear buttons disabled state based on inspector running state
function updateActionButtons(inspectorRunning) {
  const exportBtn = document.getElementById('exportBtn');
  const clearBtn = document.getElementById('clearBtn');

  if (inspectorRunning) {
    exportBtn?.classList.add('disabled');
    clearBtn?.classList.add('disabled');
  } else {
    exportBtn?.classList.remove('disabled');
    clearBtn?.classList.remove('disabled');
  }
}

// Auto-stop bildirimi gÃ¶ster (origin deÄŸiÅŸikliÄŸi vb.)
function showAutoStopBanner(reason) {
  const banner = document.getElementById('autoStopBanner');
  if (!banner) return;

  const messages = {
    'origin_change': 'Inspector stopped: Site changed',
    'injection_failed': 'Injection failed - please reload page',
    'tab_switch': 'âš ï¸ Inspecting stopped: Switched to different tab',
    'navigation': 'âš ï¸ Inspector stopped: Navigated to different site',
    'window_switch': 'âš ï¸ Inspector stopped: Switched to different window'
  };
  banner.textContent = messages[reason] || 'Inspector stopped';
  banner.classList.add('visible');

  // 5 saniye sonra gizle
  setTimeout(() => {
    banner.classList.remove('visible');
  }, 5000);

  debugLog(`Auto-stop banner shown: ${reason}`);
}

// Format timestamp
function formatTime(timestamp) {
  if (!timestamp) return '';
  const date = new Date(timestamp);
  return date.toLocaleTimeString('tr-TR', {
    hour: '2-digit',
    minute: '2-digit',
    second: '2-digit'
  });
}

// XSS korumasÄ± - HTML special karakterlerini escape et
function escapeHtml(text) {
  if (!text) return '';
  const div = document.createElement('div');
  div.textContent = text;
  return div.innerHTML;
}

// UI helper - pulsing status text (Detecting / Waiting / Pending / Loading)
function renderStatusPulse(text, tooltip) {
  const safeText = escapeHtml(text);
  if (tooltip) {
    return `<span class="has-tooltip status-pulse" data-tooltip="${escapeHtml(tooltip)}">${safeText}</span>`;
  }
  return `<span class="status-pulse">${safeText}</span>`;
}

// Format encoder type for display (e.g., "opus-wasm" â†’ "Opus (WASM)")
function formatEncoderDisplay(encoder) {
  if (!encoder) return 'Unknown';
  const lower = String(encoder).toLowerCase();

  // Generic WASM encoder types
  const detectedEncoders = {
    'opus-wasm': 'Opus (WASM)',
    'mp3-wasm': 'MP3 (WASM)',
    'aac-wasm': 'AAC (WASM)',
    'vorbis-wasm': 'Vorbis (WASM)',
    'flac-wasm': 'FLAC (WASM)',
    'pcm': 'Linear PCM'
  };

  if (detectedEncoders[lower]) {
    return detectedEncoders[lower];
  }

  // Legacy fallback - old format still in storage
  const legacyEncoders = {
    'opus-recorder': 'Opus (WASM)',
    'lamejs': 'MP3 (WASM)',
    'fdk-aac.js': 'AAC (WASM)',
    'vorbis.js': 'Vorbis (WASM)',
    'libflac.js': 'FLAC (WASM)',
    'linear-pcm': 'Linear PCM'
  };

  if (legacyEncoders[lower]) {
    return legacyEncoders[lower];
  }

  // Return as-is if not recognized
  return encoder;
}

// Detect encoder input technology from pipeline processors
// Priority: Worklet > ScriptProcessor > WebAudio > MediaStream
// Returns the "technology" that processes audio before encoding
function detectEncoderInputTechnology(processors) {
  if (!processors || processors.length === 0) return null;

  // Filter out analysers (monitors don't process audio for encoding)
  const mainChain = processors.filter(p => p.type !== 'analyser');
  if (mainChain.length === 0) return null;

  // Look for Worklet (highest priority - modern audio processing)
  const worklet = mainChain.find(p => {
    const type = (p.type || '').toLowerCase();
    return type === 'audioworkletnode' || type === 'audioworklet';
  });
  if (worklet) {
    const name = worklet.name || worklet.processorName;
    return name ? `Worklet (${name})` : 'Worklet';
  }

  // Look for ScriptProcessor (legacy audio processing)
  const scriptProc = mainChain.find(p => {
    const type = (p.type || '').toLowerCase();
    return type === 'scriptprocessor' || type === 'scriptprocessornode';
  });
  if (scriptProc) {
    return 'ScriptProcessor';
  }

  // Look for any WebAudio processing nodes (not sources)
  const webAudioNodes = mainChain.filter(p => {
    const type = (p.type || '').toLowerCase();
    // Exclude source nodes - we want processing nodes
    return !type.includes('source') && !type.includes('element');
  });

  if (webAudioNodes.length > 0) {
    // Return the last WebAudio node in the chain
    const lastNode = webAudioNodes[webAudioNodes.length - 1];
    const nodeType = formatWebAudioNodeType(lastNode.type);
    return `WebAudio (${nodeType})`;
  }

  // Only source nodes - direct stream
  const sourceNode = mainChain.find(p => {
    const type = (p.type || '').toLowerCase();
    return type.includes('source');
  });
  if (sourceNode) {
    return 'MediaStream (direct)';
  }

  return null;
}

// Format WebAudio node type to readable name
function formatWebAudioNodeType(type) {
  if (!type) return 'Unknown';
  const t = type.toLowerCase();

  const nodeNames = {
    'gain': 'Gain',
    'gainnode': 'Gain',
    'biquadfilter': 'Filter',
    'biquadfilternode': 'Filter',
    'dynamicscompressor': 'Compressor',
    'dynamicscompressornode': 'Compressor',
    'convolver': 'Convolver',
    'convolvernode': 'Convolver',
    'waveshaper': 'WaveShaper',
    'waveshapernode': 'WaveShaper',
    'panner': 'Panner',
    'pannernode': 'Panner',
    'stereopanner': 'Panner',
    'stereopannernode': 'Panner',
    'delay': 'Delay',
    'delaynode': 'Delay',
    'iirfilter': 'IIRFilter',
    'iirfilternode': 'IIRFilter'
  };

  return nodeNames[t] || type;
}

// Format jitter (seconds to ms)
function formatJitter(jitterSec) {
  if (!jitterSec) return 'N/A';
  return `${(jitterSec * 1000).toFixed(2)} ms`;
}

// Color code values
function getQualityClass(metric, value) {
  if (metric === 'jitter') {
    if (value < 0.03) return 'good';     // < 30ms
    if (value < 0.1) return 'warning';   // < 100ms
    return 'error';
  }
  if (metric === 'packetLoss') {
    if (value < 1) return 'good';        // < 1%
    if (value < 5) return 'warning';     // < 5%
    return 'error';
  }
  if (metric === 'rtt') {
    if (value < 0.15) return 'good';     // < 150ms
    if (value < 0.3) return 'warning';   // < 300ms
    return 'error';
  }
  return '';
}

// Render WebRTC stats (fixed two-column layout)
function renderRTCStats(data) {
  const container = document.getElementById('rtcContent');
  const timestamp = document.getElementById('rtcTimestamp');

  // Find active connection with send or recv
  let pc = null;
  let connCount = 0;

  if (data?.peerConnections?.length > 0) {
    connCount = data.peerConnections.length;
    pc = data.peerConnections.find(c => c.send && c.recv)
      || data.peerConnections.find(c => c.send || c.recv)
      || data.peerConnections[0];
  }

  const connInfo = connCount > 0 ? ` (${connCount})` : '';

  let sendHtml = `<div class="rtc-column">
    <div class="sub-header sub-header--rtc">
      <span class="direction-icon send">TX</span>
      Outgoing${connInfo}
    </div>`;

  let recvHtml = `<div class="rtc-column">
    <div class="sub-header sub-header--rtc">
      <span class="direction-icon recv">RX</span>
      Incoming
    </div>`;

  if (!pc) {
    sendHtml += `<table><tbody>
      <tr><td>Codec</td><td>-</td></tr>
      <tr><td>Bitrate</td><td>-</td></tr>
      <tr><td>Mode</td><td>-</td></tr>
      <tr><td>RTT</td><td>-</td></tr>
    </tbody></table></div>`;

    recvHtml += `<table><tbody>
      <tr><td>Codec</td><td>-</td></tr>
      <tr><td>Bitrate</td><td>-</td></tr>
      <tr><td>Jitter</td><td>-</td></tr>
      <tr><td>Loss</td><td>-</td></tr>
    </tbody></table></div>`;

    container.innerHTML = sendHtml + recvHtml;
    timestamp.textContent = '';
    return;
  }

  timestamp.textContent = formatTime(data.timestamp);

  const rttText = pc.rtt !== null ? `${(pc.rtt * 1000).toFixed(0)} ms` : '-';
  const rttClass = pc.rtt !== null ? getQualityClass('rtt', pc.rtt) : '';

  // Send column
  sendHtml += `<table><tbody>`;
  if (pc.send) {
    const codec = pc.send.codec?.split('/')[1] || '-';
    const bitrate = pc.send.bitrateKbps !== null ? `${pc.send.bitrateKbps} kbps` : '-';
    const mode = pc.send.opusParams ? (pc.send.opusParams.cbr === 1 ? 'CBR' : 'VBR') : '-';
    const fec = pc.send.opusParams?.useinbandfec === 1 ? '+FEC' : '';

    sendHtml += `<tr><td>Codec</td><td class="metric-value">${codec}</td></tr>`;
    sendHtml += `<tr><td>Bitrate</td><td class="metric-value">${bitrate}</td></tr>`;
    sendHtml += `<tr><td>Mode</td><td>${mode} ${fec}</td></tr>`;
  } else {
    sendHtml += `<tr><td>Codec</td><td>-</td></tr>`;
    sendHtml += `<tr><td>Bitrate</td><td>-</td></tr>`;
    sendHtml += `<tr><td>Mode</td><td>-</td></tr>`;
  }
  sendHtml += `<tr><td>RTT</td><td class="${rttClass}">${rttText}</td></tr>`;
  sendHtml += `</tbody></table></div>`;

  // Recv column
  recvHtml += `<table><tbody>`;
  if (pc.recv) {
    const codec = pc.recv.codec?.split('/')[1] || '-';
    const bitrate = pc.recv.bitrateKbps !== null ? `${pc.recv.bitrateKbps} kbps` : '-';
    const jitter = formatJitter(pc.recv.jitter);
    const jitterClass = getQualityClass('jitter', pc.recv.jitter);
    const plr = pc.recv.packetsReceived > 0 ? ((pc.recv.packetsLost / (pc.recv.packetsReceived + pc.recv.packetsLost)) * 100) : 0;
    const plrClass = getQualityClass('packetLoss', plr);

    recvHtml += `<tr><td>Codec</td><td class="metric-value">${codec}</td></tr>`;
    recvHtml += `<tr><td>Bitrate</td><td class="metric-value">${bitrate}</td></tr>`;
    recvHtml += `<tr><td>Jitter</td><td class="${jitterClass}">${jitter}</td></tr>`;
    recvHtml += `<tr><td>Loss</td><td class="${plrClass}">${plr.toFixed(1)}%</td></tr>`;
  } else {
    recvHtml += `<tr><td>Codec</td><td>-</td></tr>`;
    recvHtml += `<tr><td>Bitrate</td><td>-</td></tr>`;
    recvHtml += `<tr><td>Jitter</td><td>-</td></tr>`;
    recvHtml += `<tr><td>Loss</td><td>-</td></tr>`;
  }
  recvHtml += `</tbody></table></div>`;

  container.innerHTML = sendHtml + recvHtml;
}

// DSP field configuration (OCP: add new DSP types without modifying loop)
const DSP_FIELDS = [
  { key: 'echoCancellation', label: 'Echo Cancel' },
  { key: 'autoGainControl', label: 'Auto Gain' },
  { key: 'noiseSuppression', label: 'Noise Supp' },
  { key: 'voiceIsolation', label: 'Voice Isolation' }
];

// Render getUserMedia stats (fixed layout)
function renderGUMStats(data) {
  const container = document.getElementById('gumContent');
  const timestamp = document.getElementById('gumTimestamp');

  let html = `<table><tbody>`;

  if (!data || !data.settings) {
    html += `<tr><td>Rate</td><td>-</td></tr>`;
    html += `<tr><td>Channels</td><td>-</td></tr>`;
    html += `<tr><td>Bit Depth</td><td>-</td></tr>`;
    html += `<tr><td>In Latency</td><td>-</td></tr>`;
    // OCP: DSP placeholder satÄ±rlarÄ± DSP_FIELDS'den Ã¼retiliyor
    DSP_FIELDS.forEach(field => {
      html += `<tr><td>${field.label}</td><td>-</td></tr>`;
    });
    html += `</tbody></table>`;
    container.innerHTML = html;
    timestamp.textContent = '';
    return;
  }

  timestamp.textContent = formatTime(data.timestamp);
  const s = data.settings;

  html += `<tr><td>Rate</td><td class="metric-value">${s.sampleRate || '-'} Hz</td></tr>`;
  html += `<tr><td>Channels</td><td>${s.channelCount || '?'}ch</td></tr>`;
  html += `<tr><td>Bit Depth</td><td>${s.sampleSize || '?'}bit</td></tr>`;

  // Input Latency (mikrofon giriÅŸ gecikmesi - AudioContext output latency'den farklÄ±)
  const inLatency = s.latency ? `${(s.latency * 1000).toFixed(1)} ms` : '-';
  html += `<tr><td>In Latency</td><td>${inLatency}</td></tr>`;

  // DSP satÄ±rlarÄ± (her Ã¶zellik ayrÄ± satÄ±rda)
  DSP_FIELDS.forEach(field => {
    const value = s[field.key];
    const display = value ? 'âœ“' : '-';
    const cls = value ? 'good' : '';
    html += `<tr><td>${field.label}</td><td class="${cls}">${display}</td></tr>`;
  });

  html += `</tbody></table>`;
  container.innerHTML = html;
}


// Determine AudioContext purpose based on evidence-based model
// IMPORTANT: We do NOT show "Recording" label here - that's misleading
// Recording status is shown ONLY in MediaRecorder section where we have real evidence
// AudioContext can only show what it's configured for, not what's actually happening
// Note: WASM encoder is shown as independent signal, not attached to context
// Tooltip explains the detection is point-in-time, not guaranteed current
function getContextPurpose(ctx) {
  // 1. Mikrofon baÄŸlÄ± - GÄ°DEN SES (outgoing audio)
  if (ctx.pipeline?.inputSource === 'microphone') {
    return {
      icon: 'ðŸŽ¤',
      label: 'Mic Input',
      tooltip: 'Microphone stream detected - outgoing audio'
    };
  }

  // 2. Remote audio baÄŸlÄ± - GELEN SES (incoming audio) - filtrelenecek
  if (ctx.pipeline?.inputSource === 'remote') {
    return {
      icon: 'ðŸ“¥',
      label: 'Remote Input',
      tooltip: 'Remote stream detected - incoming audio from peer'
    };
  }

  // 3. MediaStreamDestination = audio routed to stream - GÄ°DEN SES
  if (ctx.pipeline?.destinationType === DESTINATION_TYPES.MEDIA_STREAM) {
    return {
      icon: 'ðŸ“¤',
      label: 'Stream Output',
      tooltip: 'MediaStreamDestination created - audio routed to stream'
    };
  }

  // 4. AnalyserNode = VU Meter / visualizer - belirsiz, filtrelenecek
  if (ctx.pipeline?.processors?.some(p => p.type === 'analyser')) {
    return {
      icon: 'ðŸ“Š',
      label: 'VU Meter',
      tooltip: 'AnalyserNode detected - audio visualization'
    };
  }

  // 5. Default â†’ Page Audio (no microphone capture)
  return { icon: 'ðŸŽµ', label: 'Page Audio', tooltip: 'VU meter, audio effects, playback analysis etc.' };
}

/**
 * Filter AudioContext'leri - giden ses + aktif/yeni context'leri dÃ¶ndÃ¼r
 * Giden ses: Mic Input + Stream Output â†’ her zaman gÃ¶ster
 * DiÄŸerleri (VU Meter, Other): sadece running state veya son 5 saniyede oluÅŸturulmuÅŸ ise gÃ¶ster
 *
 * Ã–ncelik: Mic Input > Stream Output > diÄŸerleri
 * EÄŸer Mic Input varsa, sadece Mic Input context'lerini gÃ¶ster (Stream Output genellikle hazÄ±rlÄ±k)
 */
function filterOutgoingContexts(contexts) {
  const now = Date.now();
  const RECENT_THRESHOLD_MS = 5000; // 5 saniye

  const getContextTimestamp = (ctx) => ctx.pipeline?.timestamp || ctx.static?.timestamp || 0;

  // â•â•â• DEBUG: Initial contexts â•â•â•
  console.log(`[AudioInspector] ðŸ” filterOutgoingContexts: input has ${contexts.length} context(s)`);
  contexts.forEach((ctx, i) => {
    const purpose = getContextPurpose(ctx);
    console.log(`[AudioInspector] ðŸ” Input Context[${i}]: ${ctx.contextId} - purpose="${purpose.label}", inputSource=${ctx.pipeline?.inputSource}, state=${ctx.static?.state}`);
  });

  // Ä°lk geÃ§iÅŸ: tÃ¼m potansiyel context'leri topla
  const candidates = contexts.filter(ctx => {
    const purpose = getContextPurpose(ctx);

    // Outgoing audio - her zaman aday
    if (purpose.label === 'Mic Input' || purpose.label === 'Stream Output') {
      return true;
    }

    // Page Audio/VU Meter - sadece running veya yeni ise aday
    const isRunning = ctx.static?.state === 'running';
    const isRecent = (now - (ctx.static?.timestamp || 0)) < RECENT_THRESHOLD_MS;

    return isRunning || isRecent;
  });

  console.log(`[AudioInspector] ðŸ” filterOutgoingContexts: ${candidates.length} candidate(s) after first pass`);

  // Mic Input varsa, sadece Mic Input context'lerini dÃ¶ndÃ¼r
  // Bu, "hazÄ±rlÄ±k" context'lerini (sadece destination) gizler
  const micInputContexts = candidates.filter(ctx =>
    getContextPurpose(ctx).label === 'Mic Input'
  );

  console.log(`[AudioInspector] ðŸ” filterOutgoingContexts: ${micInputContexts.length} Mic Input context(s) found`);

  if (micInputContexts.length > 0) {
    const sortedMicInputs = [...micInputContexts].sort(
      (a, b) => getContextTimestamp(b) - getContextTimestamp(a)
    );
    console.log(`[AudioInspector] ðŸ” filterOutgoingContexts: returning NEWEST Mic Input context: ${sortedMicInputs[0].contextId}`);
    return [sortedMicInputs[0]];
  }

  // Mic Input yoksa tÃ¼m adaylarÄ± dÃ¶ndÃ¼r
  console.log(`[AudioInspector] ðŸ” filterOutgoingContexts: no Mic Input, returning all ${candidates.length} candidate(s)`);
  return candidates;
}

function filterConnectionsByContext(connections, contexts) {
  if (!connections || connections.length === 0) return [];
  if (!contexts || contexts.length === 0) return connections;

  const contextIds = new Set(
    contexts.map(ctx => ctx.contextId).filter(Boolean)
  );
  if (contextIds.size === 0) return connections;

  const hasContextIds = connections.some(conn => conn.contextId);
  if (!hasContextIds) return connections;

  return connections.filter(conn => contextIds.has(conn.contextId));
}

function mapNodeTypeToProcessorType(nodeType) {
  if (!nodeType || typeof nodeType !== 'string') return null;
  switch (nodeType) {
    case 'AudioWorklet': return 'audioWorkletNode';
    case 'ScriptProcessor': return 'scriptProcessor';
    case 'Analyser': return 'analyser';
    case 'Gain': return 'gain';
    case 'BiquadFilter': return 'biquadFilter';
    case 'DynamicsCompressor': return 'dynamicsCompressor';
    case 'Oscillator': return 'oscillator';
    case 'Delay': return 'delay';
    case 'Convolver': return 'convolver';
    case 'WaveShaper': return 'waveShaper';
    case 'Panner': return 'panner';
    default:
      // Best-effort: "FooBar" â†’ "fooBar"
      return nodeType.charAt(0).toLowerCase() + nodeType.slice(1);
  }
}

function isDestinationNodeType(nodeType) {
  if (!nodeType || typeof nodeType !== 'string') return false;
  return nodeType === 'AudioDestination' ||
    nodeType === 'MediaStreamAudioDestination' ||
    nodeType === 'MediaStreamDestination';
}

/**
 * Derive the main Audio Path chain from the connection graph.
 * This avoids "stacking" old processors across multiple recordings by rendering
 * only currently-connected nodes (connections are pruned on disconnect()).
 *
 * Falls back to pipeline processors if graph path cannot be resolved.
 *
 * @param {Array} connections
 * @param {Object} ctx
 * @returns {Array}
 */
function deriveMainChainProcessorsFromConnections(connections, ctx) {
  if (!connections || connections.length === 0) return [];

  // nodeId â†’ pipeline processor (has processorName/options for worklets)
  const pipelineByNodeId = new Map();
  const pipelineProcessors = ctx?.pipeline?.processors || [];
  for (const p of pipelineProcessors) {
    if (p?.nodeId) {
      pipelineByNodeId.set(p.nodeId, p);
    }
  }

  // nodeId â†’ human-readable node type (from connect() hook)
  const nodeTypeById = new Map();
  const edges = new Map(); // sourceId â†’ destId[]

  for (const c of connections) {
    if (!c?.sourceId || !c?.destId) continue;
    if (typeof c.destType === 'string' && c.destType.startsWith('AudioParam(')) continue;

    if (c.sourceType) nodeTypeById.set(c.sourceId, c.sourceType);
    if (c.destType) nodeTypeById.set(c.destId, c.destType);

    const list = edges.get(c.sourceId) || [];
    list.push(c.destId);
    edges.set(c.sourceId, list);
  }

  const startIds = connections
    .filter(c => c?.sourceType === 'MediaStreamAudioSource' && c?.sourceId)
    .map(c => c.sourceId);

  const destIds = new Set(
    connections
      .filter(c => isDestinationNodeType(c?.destType) && c?.destId)
      .map(c => c.destId)
  );

  const findPath = (startId) => {
    const queue = [startId];
    const prev = new Map();
    prev.set(startId, null);

    while (queue.length > 0) {
      const cur = queue.shift();
      if (destIds.has(cur)) {
        // Reconstruct path
        const path = [];
        let n = cur;
        while (n) {
          path.push(n);
          n = prev.get(n);
        }
        path.reverse();
        return path;
      }

      const neighbors = edges.get(cur) || [];
      for (const next of neighbors) {
        if (!next || prev.has(next)) continue;
        prev.set(next, cur);
        queue.push(next);
      }
    }
    return null;
  };

  let bestPath = null;
  for (const startId of startIds) {
    const path = findPath(startId);
    if (!path) continue;
    if (!bestPath || path.length < bestPath.length) {
      bestPath = path;
    }
  }

  if (!bestPath || bestPath.length < 2) return [];

  const processors = [];
  for (const nodeId of bestPath) {
    const nodeType = nodeTypeById.get(nodeId);
    if (nodeType === 'MediaStreamAudioSource') continue;
    if (isDestinationNodeType(nodeType)) continue;

    const fromPipeline = pipelineByNodeId.get(nodeId);
    if (fromPipeline) {
      if (fromPipeline.type !== 'analyser') {
        processors.push(fromPipeline);
      }
      continue;
    }

    const mappedType = mapNodeTypeToProcessorType(nodeType);
    if (!mappedType || mappedType === 'analyser') continue;

    const entry = { type: mappedType, nodeId, timestamp: Date.now() };
    if (mappedType === 'audioWorkletNode') {
      entry.processorName = '?';
    }
    processors.push(entry);
  }

  return processors;
}

/**
 * Get processor key for grouping - same key = same type
 * Used to group consecutive identical processors
 */
function getProcessorKey(proc) {
  switch (proc.type) {
    case 'gain': return 'gain';
    case 'biquadFilter': return 'biquadFilter'; // TÃ¼m filtreler gruplanÄ±r
    case 'dynamicsCompressor': return 'compressor';
    case 'oscillator': return 'oscillator'; // TÃ¼m osilatÃ¶rler gruplanÄ±r
    case 'delay': return 'delay'; // TÃ¼m delay'ler gruplanÄ±r
    case 'audioWorkletNode': return `worklet-${proc.processorName || '?'}`;
    case 'scriptProcessor': return 'scriptProcessor';
    case 'audioWorklet': return `audioWorklet-${proc.moduleUrl || '?'}`;
    case 'analyser': return 'analyser';
    default: return proc.type;
  }
}

/**
 * Group consecutive identical processors
 * Example: [Gain, Gain, Filter, Gain] â†’ [Gain(x2), Filter, Gain(x1)]
 * Note: Only consecutive, not total count
 */
function groupConsecutiveProcessors(processors) {
  if (!processors || processors.length === 0) return [];

  const grouped = [];
  for (const proc of processors) {
    const key = getProcessorKey(proc);
    const last = grouped[grouped.length - 1];

    if (last && last.key === key) {
      last.count++;
    } else {
      grouped.push({ ...proc, key, count: 1 });
    }
  }
  return grouped;
}

/**
 * MediaStreamSource tekrarlarÄ±nÄ± (aynÄ± zincirde ardÄ±ÅŸÄ±k ya da tekrarlÄ±) tek girdiye indirger
 * GÃ¶rselde "mediaStreamSource(x3)" yerine tek kaynak gÃ¶sterir.
 */
function dedupeMediaStreamSources(processors) {
  if (!processors || processors.length === 0) return [];

  let seen = false;
  return processors.filter(proc => {
    if (proc.type !== 'mediaStreamSource') return true;
    if (seen) return false;
    seen = true;
    return true;
  });
}

/**
 * Extract Processing and Effects info from processors
 *
 * Processing: AudioWorklet veya ScriptProcessor (ses iÅŸleme)
 * Effects: Sadece GERÃ‡EK ses efektleri (Filter, Reverb, Delay, vb.)
 *
 * NOT effect olanlar:
 * - Volume/Gain: Ses seviyesi kontrolÃ¼, effect deÄŸil
 * - Processor: Zaten Processing'de gÃ¶steriliyor
 * - Analyzer: Monitoring, effect deÄŸil
 *
 * @param {Array} mainProcessors - Main chain processors
 * @param {Array} monitors - Analyser nodes
 * @returns {{ processingText: string, effectsText: string }}
 */
function extractProcessingInfo(mainProcessors, monitors) {
  let processingText = '';
  let effectsText = '';

  // Processing: Worklet veya ScriptProcessor bilgisi
  const worklet = mainProcessors.find(p => p.type === 'audioWorkletNode');
  const scriptProc = mainProcessors.find(p => p.type === 'scriptProcessor');

  if (worklet) {
    const procName = worklet.processorName || 'processor';
    // KÄ±sa isim: passthrough-processor â†’ passthrough
    const shortName = procName.replace(/-processor$/, '');
    processingText = `Worklet (${shortName})`;
  } else if (scriptProc) {
    processingText = 'ScriptProcessor';
  }

  // Effects: SADECE gerÃ§ek ses efektleri
  // Volume/Gain = effect DEÄžÄ°L (ses seviyesi kontrolÃ¼)
  // Processor = effect DEÄžÄ°L (zaten Processing'de)
  // Analyzer = effect DEÄžÄ°L (monitoring)
  const effectNodes = [];

  mainProcessors.forEach(p => {
    // GerÃ§ek efektler
    if (p.type === 'biquadFilter') effectNodes.push('Filter');
    else if (p.type === 'convolver') effectNodes.push('Reverb');
    else if (p.type === 'delay') effectNodes.push('Delay');
    else if (p.type === 'dynamicsCompressor') effectNodes.push('Compressor');
    else if (p.type === 'waveShaper') effectNodes.push('Distortion');
    else if (p.type === 'stereoPanner') effectNodes.push('Panner');
    else if (p.type === 'panner') effectNodes.push('3D Panner');
    // NOT: gain, audioWorkletNode, scriptProcessor, analyser â†’ effect DEÄžÄ°L
  });

  // Unique efektler - boÅŸsa gÃ¶sterme
  const uniqueEffects = [...new Set(effectNodes)];
  if (uniqueEffects.length > 0) {
    effectsText = uniqueEffects.join(', ');
  }

  return { processingText, effectsText };
}

/**
 * Render Audio Path as nested ASCII tree with tooltips
 * Tree yapÄ±sÄ± (Encoder son node - AudioContext'in Ã§Ä±kÄ±ÅŸÄ±):
 *   Microphone                     [tooltip: MediaStreamAudioSourceNode]
 *   â””â”€â”€ Processor (passthrough)    [tooltip: AudioWorkletNode]
 *       â””â”€â”€ Volume (pass)          [tooltip: GainNode]
 *           â”œâ”€â”€ Encoder (output)   [tooltip: Audio output to encoding pipeline]
 *           â””â”€â”€ Analyzer           [tooltip: AnalyserNode - monitoring]
 *
 * @param {Array} mainProcessors - Main chain processors
 * @param {Array} monitors - Analyser nodes (side-tap, monitoring)
 * @param {string} inputSource - Input source type ('microphone', 'remote', etc.)
 * @returns {string} HTML string for tree
 */
function renderAudioPathTree(mainProcessors, monitors, inputSource) {
  if ((!mainProcessors || mainProcessors.length === 0) && !inputSource) {
    return '<div class="no-data">No audio path</div>';
  }

  // Build nested tree structure
  // Her node: { label, param?, tooltip?, children: [], isMonitor?, isEncoder? }
  const buildNestedTree = () => {
    // Root: Input source (tooltip ile birlikte)
    const rootLabel = inputSource
      ? inputSource.charAt(0).toUpperCase() + inputSource.slice(1)
      : 'Source';
    const rootTooltip = inputSource === 'microphone'
      ? 'MediaStreamAudioSourceNode'
      : inputSource === 'remote'
        ? 'MediaStreamAudioSourceNode (remote)'
        : 'AudioSourceNode';
    const root = { label: rootLabel, tooltip: rootTooltip, children: [], isRoot: true };

    // Chain processors (mediaStreamSource hariÃ§ - zaten root olarak gÃ¶steriyoruz)
    const chainProcessors = (mainProcessors || []).filter(p => p.type !== 'mediaStreamSource');

    // Nested yapÄ±: her processor bir Ã¶ncekinin child'Ä±
    let currentParent = root;
    let lastProcessorNode = root;

    chainProcessors.forEach((proc) => {
      const formatted = formatProcessorForTree(proc);
      const node = {
        label: formatted.label,
        param: formatted.param,
        tooltip: formatted.tooltip,
        children: []
      };

      currentParent.children.push(node);
      lastProcessorNode = node;
      currentParent = node; // Sonraki processor bu node'un child'Ä± olacak
    });

    // â•â•â• ENCODER NODE â•â•â•
    // Tree'nin ana hat Ã§Ä±kÄ±ÅŸÄ± daima Encoder - AudioContext burada biter
    // Encoding detaylarÄ± ENCODING bÃ¶lÃ¼mÃ¼nde gÃ¶sterilir
    const encoderNode = {
      label: 'Encoder',
      param: 'output',
      tooltip: 'Audio output â†’ Encoding pipeline (see ENCODING section)',
      children: [],
      isEncoder: true
    };

    // â•â•â• ANALYZER NODES â•â•â• (Monitoring - ana akÄ±ÅŸtan ayrÄ±)
    const analyzerNodes = monitors.map((mon) => {
      const formatted = formatProcessorForTree(mon);
      return {
        label: formatted.label,
        param: formatted.param,
        tooltip: formatted.tooltip + ' (monitoring tap)',
        children: [],
        isMonitor: true
      };
    });

    // Encoder ve Analyzer'Ä± son processor'un children'Ä± olarak ekle
    // Encoder Ã¶nce (main output), Analyzer sonra (monitoring)
    lastProcessorNode.children.push(encoderNode);
    analyzerNodes.forEach(an => lastProcessorNode.children.push(an));

    return root;
  };

  const tree = buildNestedTree();

  // Recursive render function
  // parentVerticals: hangi depth'lerde dikey Ã§izgi devam etmeli (array of booleans)
  const renderNode = (node, depth, isLastChild, parentVerticals) => {
    let html = '';

    const classes = ['audio-tree-node'];
    if (node.isRoot) classes.push('tree-root');
    if (node.isMonitor) classes.push('tree-node-monitor');
    if (node.isEncoder) classes.push('tree-node-encoder');

    html += `<div class="${classes.join(' ')}">`;

    // Branch segments (depth > 0 iÃ§in)
    if (depth > 0) {
      html += '<div class="tree-branch">';

      // Ã–nceki depth'lerdeki dikey Ã§izgiler
      for (let d = 0; d < depth - 1; d++) {
        if (parentVerticals[d]) {
          html += '<div class="tree-segment tree-segment--vertical"></div>';
        } else {
          html += '<div class="tree-segment"></div>';
        }
      }

      // Son segment: corner (â””) veya fork (â”œ)
      const segmentClass = isLastChild ? 'tree-segment--corner' : 'tree-segment--fork';
      html += `<div class="tree-segment ${segmentClass}"></div>`;

      html += '</div>';
    }

    // Node iÃ§eriÄŸi - tooltip ile birlikte
    // Label + param birlikte tooltip span'Ä± iÃ§inde
    const displayText = node.param
      ? `${escapeHtml(node.label)} (${escapeHtml(node.param)})`
      : escapeHtml(node.label);

    if (node.tooltip) {
      html += `<span class="tree-node-name has-tooltip" data-tooltip="${escapeHtml(node.tooltip)}">${displayText}</span>`;
    } else {
      html += `<span class="tree-node-name">${displayText}</span>`;
    }

    html += '</div>';

    // Children recursive render
    const children = node.children || [];
    children.forEach((child, index) => {
      const childIsLast = index === children.length - 1;

      // parentVerticals gÃ¼ncelle: bu node'un altÄ±nda daha child varsa dikey Ã§izgi devam eder
      const newParentVerticals = [...parentVerticals];
      if (depth > 0) {
        newParentVerticals[depth - 1] = !isLastChild;
      }

      html += renderNode(child, depth + 1, childIsLast, newParentVerticals);
    });

    return html;
  };

  let html = '<div class="audio-tree">';
  html += renderNode(tree, 0, true, []);
  html += '</div>';

  return html;
}

/**
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * AUDIO NODE USER-FRIENDLY MAPPING
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 *
 * Tree'de gÃ¶sterilecek: KullanÄ±cÄ± dostu isimler
 * Tooltip'te gÃ¶sterilecek: Teknik node isimleri
 *
 * Format: {
 *   label: string,      // Tree'de gÃ¶sterilen kullanÄ±cÄ± dostu isim
 *   tooltip: string,    // Hover'da gÃ¶sterilen teknik isim
 *   getParam?: (proc) => string  // Parametre hesaplama (opsiyonel)
 * }
 */
const AUDIO_NODE_DISPLAY_MAP = {
  // â•â•â• SOURCE NODES â•â•â•
  mediaStreamSource: {
    label: 'Microphone',
    tooltip: 'MediaStreamAudioSourceNode'
  },
  mediaElementSource: {
    label: 'Media Player',
    tooltip: 'MediaElementAudioSourceNode',
    getParam: (proc) => proc.mediaType || null // 'audio' veya 'video'
  },
  bufferSource: {
    label: 'Audio Buffer',
    tooltip: 'AudioBufferSourceNode',
    getParam: (proc) => proc.loop ? 'loop' : null
  },
  oscillator: {
    label: 'Tone Generator',
    tooltip: 'OscillatorNode',
    getParam: (proc) => {
      // sine, square, sawtooth, triangle, custom
      const typeMap = {
        'sine': 'sine',
        'square': 'square',
        'sawtooth': 'saw',
        'triangle': 'tri',
        'custom': 'custom'
      };
      return typeMap[proc.oscillatorType] || proc.oscillatorType || null;
    }
  },
  constantSource: {
    label: 'DC Offset',
    tooltip: 'ConstantSourceNode'
  },

  // â•â•â• EFFECT / PROCESSING NODES â•â•â•
  gain: {
    label: 'Volume',
    tooltip: 'GainNode',
    getParam: (proc) => {
      const gain = proc.gainValue ?? proc.gain;
      if (gain === undefined || gain === null) return null;

      // Gain deÄŸerine gÃ¶re kullanÄ±cÄ± dostu parametre
      if (gain === 1 || Math.abs(gain - 1) < 0.001) {
        return 'pass'; // Ses deÄŸiÅŸmeden geÃ§iyor
      }
      if (gain === 0) {
        return 'muted'; // Ses tamamen kapalÄ±
      }
      if (gain < 1) {
        // dB olarak gÃ¶ster (negative)
        const dB = 20 * Math.log10(gain);
        return `${dB.toFixed(0)}dB`;
      }
      // gain > 1, amplification
      const dB = 20 * Math.log10(gain);
      return `+${dB.toFixed(0)}dB`;
    }
  },
  biquadFilter: {
    label: 'Filter',
    tooltip: 'BiquadFilterNode',
    getParam: (proc) => {
      // lowpass, highpass, bandpass, lowshelf, highshelf, peaking, notch, allpass
      const typeMap = {
        'lowpass': 'LP',
        'highpass': 'HP',
        'bandpass': 'BP',
        'lowshelf': 'LS',
        'highshelf': 'HS',
        'peaking': 'peak',
        'notch': 'notch',
        'allpass': 'AP'
      };
      const shortType = typeMap[proc.filterType] || proc.filterType;
      // Frekans bilgisi varsa ekle
      if (proc.frequency) {
        const freq = proc.frequency >= 1000
          ? `${(proc.frequency / 1000).toFixed(1)}k`
          : `${Math.round(proc.frequency)}`;
        return `${shortType} ${freq}Hz`;
      }
      return shortType || null;
    }
  },
  dynamicsCompressor: {
    label: 'Compressor',
    tooltip: 'DynamicsCompressorNode',
    getParam: (proc) => {
      // Threshold ve ratio varsa gÃ¶ster
      if (proc.threshold !== undefined && proc.ratio !== undefined) {
        return `${proc.threshold}dB ${proc.ratio}:1`;
      }
      if (proc.threshold !== undefined) {
        return `${proc.threshold}dB`;
      }
      return null;
    }
  },
  convolver: {
    label: 'Reverb',
    tooltip: 'ConvolverNode',
    getParam: (proc) => proc.normalize === false ? 'raw' : null
  },
  delay: {
    label: 'Delay',
    tooltip: 'DelayNode',
    getParam: (proc) => {
      const time = proc.delayTime ?? proc.maxDelayTime;
      if (time !== undefined) {
        // ms olarak gÃ¶ster
        const ms = time * 1000;
        return ms >= 1000 ? `${(ms / 1000).toFixed(1)}s` : `${Math.round(ms)}ms`;
      }
      return null;
    }
  },
  waveShaper: {
    label: 'Distortion',
    tooltip: 'WaveShaperNode',
    getParam: (proc) => {
      // oversample: none, 2x, 4x
      if (proc.oversample && proc.oversample !== 'none') {
        return proc.oversample;
      }
      return null;
    }
  },
  stereoPanner: {
    label: 'Panner',
    tooltip: 'StereoPannerNode',
    getParam: (proc) => {
      const pan = proc.pan;
      if (pan === undefined || pan === null) return null;
      if (pan === 0 || Math.abs(pan) < 0.01) return 'center';
      if (pan < 0) return `L${Math.abs(Math.round(pan * 100))}%`;
      return `R${Math.round(pan * 100)}%`;
    }
  },
  panner: {
    label: '3D Panner',
    tooltip: 'PannerNode',
    getParam: (proc) => {
      // panningModel: equalpower, HRTF
      const modelMap = { 'equalpower': 'EQ', 'HRTF': 'HRTF' };
      return modelMap[proc.panningModel] || null;
    }
  },
  iirFilter: {
    label: 'IIR Filter',
    tooltip: 'IIRFilterNode'
  },

  // â•â•â• ANALYSIS NODES â•â•â•
  analyser: {
    label: 'Analyzer',
    tooltip: 'AnalyserNode',
    getParam: (proc) => {
      if (proc.fftSize) {
        return `${proc.fftSize}pt`;
      }
      return null;
    }
  },

  // â•â•â• CHANNEL NODES â•â•â•
  channelSplitter: {
    label: 'Splitter',
    tooltip: 'ChannelSplitterNode',
    getParam: (proc) => proc.numberOfOutputs ? `${proc.numberOfOutputs}ch` : null
  },
  channelMerger: {
    label: 'Merger',
    tooltip: 'ChannelMergerNode',
    getParam: (proc) => proc.numberOfInputs ? `${proc.numberOfInputs}ch` : null
  },

  // â•â•â• WORKLET / SCRIPT NODES â•â•â•
  audioWorkletNode: {
    label: 'Processor',
    tooltip: 'AudioWorkletNode',
    getParam: (proc) => {
      if (!proc.processorName) return null;
      // passthrough-processor â†’ passthrough
      // opus-encoder-processor â†’ opus
      // vad-processor â†’ vad
      let name = proc.processorName
        .replace(/-processor$/, '')
        .replace(/-encoder$/, '')
        .replace(/-worklet$/, '');
      // Encoder isimleri iÃ§in Ã¶zel mapping
      const encoderMap = {
        'opus': 'Opus',
        'mp3': 'MP3',
        'ogg': 'OGG',
        'vorbis': 'Vorbis',
        'aac': 'AAC',
        'flac': 'FLAC',
        'wav': 'WAV',
        'pcm': 'PCM'
      };
      return encoderMap[name.toLowerCase()] || name;
    }
  },
  scriptProcessor: {
    label: 'Processor',
    tooltip: 'ScriptProcessorNode (deprecated)',
    getParam: (proc) => {
      if (proc.bufferSize) {
        return `${proc.bufferSize}`;
      }
      return null;
    }
  },

  // â•â•â• DESTINATION NODES â•â•â•
  mediaStreamDestination: {
    label: 'Stream Output',
    tooltip: 'MediaStreamAudioDestinationNode'
  },
  destination: {
    label: 'Speakers',
    tooltip: 'AudioDestinationNode'
  }
};

/**
 * Format processor for tree display
 * Returns user-friendly label + technical tooltip
 *
 * @param {Object} proc - Processor object from pipeline
 * @returns {{ label: string, param: string|null, tooltip: string }}
 */
function formatProcessorForTree(proc) {
  const mapping = AUDIO_NODE_DISPLAY_MAP[proc.type];

  if (mapping) {
    const param = mapping.getParam ? mapping.getParam(proc) : null;
    return {
      label: mapping.label,
      param,
      tooltip: mapping.tooltip
    };
  }

  // Fallback for unknown types
  // CamelCase to readable: "myCustomNode" â†’ "My Custom Node"
  const readableType = proc.type
    ? proc.type.replace(/([A-Z])/g, ' $1').replace(/^./, s => s.toUpperCase()).trim()
    : 'Unknown';

  return {
    label: readableType,
    param: null,
    tooltip: proc.type || 'Unknown AudioNode'
  };
}

/**
 * Format a single processor for display (Option B - with parameters)
 * Includes count suffix if > 1
 * Returns object with { name, params, tooltip } for flexible rendering
 */
function formatProcessor(proc) {
  const suffix = proc.count > 1 ? `(x${proc.count})` : '';
  let name = '';
  let params = '';  // Short parameter display
  let tooltip = ''; // Full details on hover

  switch (proc.type) {
    case 'scriptProcessor':
      name = `ScriptProcessor${suffix}`;
      // bufferSize artÄ±k ayrÄ± satÄ±rda gÃ¶steriliyor (renderACStats'ta)
      params = '';
      tooltip = `Input: ${proc.inputChannels || '?'}ch, Output: ${proc.outputChannels || '?'}ch`;
      break;
    case 'audioWorklet':
      name = `AudioWorklet${suffix}`;
      break;
    case 'audioWorkletNode':
      // Encoder worklet'ler iÃ§in sadece "AudioWorklet âœ“" gÃ¶ster
      // Processor detayÄ± zaten ENCODING bÃ¶lÃ¼mÃ¼nde gÃ¶rÃ¼nÃ¼yor
      const nameLC = (proc.processorName || '').toLowerCase();
      const isEncoderWorklet = ['encoder', 'opus', 'ogg', 'voice-processor', 'audio-encoder'].some(
        keyword => nameLC.includes(keyword)
      );
      if (isEncoderWorklet) {
        name = `AudioWorklet âœ“${suffix}`;
        tooltip = `Encoder: ${proc.processorName}`;
      } else {
        name = `Worklet${suffix}`;
        params = proc.processorName || '?';
        tooltip = formatWorkletOptions(proc.processorName, proc.options);
      }
      break;
    case 'gain':
      name = `Gain${suffix}`;
      // GainNode.gain.value ÅŸu an capture edilmiyor (runtime'da deÄŸiÅŸebilir)
      break;
    case 'biquadFilter':
      name = `Filter${suffix}`;
      params = proc.filterType || 'lowpass';
      break;
    case 'dynamicsCompressor':
      name = `Compressor${suffix}`;
      break;
    case 'oscillator':
      name = `Osc${suffix}`;
      params = proc.oscillatorType || 'sine';
      break;
    case 'delay':
      name = `Delay${suffix}`;
      params = `${proc.maxDelayTime || '?'}s`;
      break;
    case 'convolver':
      name = `Convolver${suffix}`;
      break;
    case 'waveShaper':
      name = `WaveShaper${suffix}`;
      params = proc.oversample !== 'none' ? proc.oversample : '';
      break;
    case 'panner':
      name = `Panner${suffix}`;
      params = proc.panningModel || 'equalpower';
      break;
    case 'analyser':
      name = `Analyser${suffix}`;
      break;
    default:
      name = `${proc.type}${suffix}`;
  }

  return { name, params, tooltip };
}

/**
 * Render chain as vertical format (one processor per line)
 * Example:
 *   Gain(x3)
 *   â†“
 *   Convolver
 *   â†“
 *   Filter(lowpass)
 * @param {Array} groupedProcessors - Grouped processors from groupConsecutiveProcessors
 * @returns {string} HTML string
 */
function renderChain(groupedProcessors) {
  if (!groupedProcessors || groupedProcessors.length === 0) {
    return '<span class="detail-value">-</span>';
  }

  const nodes = groupedProcessors.map(formatProcessor);

  // Vertical format: each processor on its own line with arrow separator
  let html = '<div class="chain-vertical chain-pipeline">';
  nodes.forEach((node, i) => {
    let text = node.name;
    if (node.params) {
      text += `(${node.params})`;
    }

    html += '<div class="chain-node">';
    if (node.tooltip) {
      html += `<span class="chain-node-name has-tooltip" data-tooltip="${escapeHtml(node.tooltip)}">${text}</span>`;
    } else {
      html += `<span class="chain-node-name">${text}</span>`;
    }
    html += '</div>';

    // Add arrow between nodes (not after last)
    if (i < nodes.length - 1) {
      html += '<span class="chain-arrow">â†“</span>';
    }
  });
  html += '</div>';

  return html;
}

/**
 * Format AudioWorkletNode options for tooltip display
 * @param {string} processorName
 * @param {Object} options
 * @returns {string}
 */
function formatWorkletOptions(processorName, options) {
  if (!options) return processorName || 'AudioWorklet';

  const parts = [];

  // numberOfInputs/Outputs
  if (options.numberOfInputs !== undefined) parts.push(`inputs: ${options.numberOfInputs}`);
  if (options.numberOfOutputs !== undefined) parts.push(`outputs: ${options.numberOfOutputs}`);

  // outputChannelCount
  if (options.outputChannelCount) {
    parts.push(`outCh: [${options.outputChannelCount.join(',')}]`);
  }

  // processorOptions - Ã¶nemli parametreleri Ã§Ä±kar
  if (options.processorOptions) {
    const po = options.processorOptions;
    if (po.sampleRate) parts.push(`rate: ${po.sampleRate}`);
    if (po.channels) parts.push(`ch: ${po.channels}`);
    if (po.bitRate) parts.push(`bitrate: ${(po.bitRate / 1000).toFixed(0)}k`);
    if (po.frameSize) parts.push(`frame: ${po.frameSize}`);
    if (po.complexity !== undefined) parts.push(`complexity: ${po.complexity}`);
  }

  return parts.length > 0 ? parts.join(', ') : processorName || 'AudioWorklet';
}


// Render AudioContext stats - supports multiple contexts
// Design: Option B - Context Info + Audio Path + Monitor (separated)
function renderACStats(contexts, audioConnections = null) {
  const container = document.getElementById('acContent');
  const timestamp = document.getElementById('acTimestamp');

  // Handle both array and single object (backwards compat)
  if (!contexts || (Array.isArray(contexts) && contexts.length === 0)) {
    container.innerHTML = '<div class="no-data">No context</div>';
    timestamp.textContent = '';
    return;
  }

  // Convert to array if single object
  let contextArray = Array.isArray(contexts) ? contexts : [contexts];

  // Sadece giden ses context'lerini gÃ¶ster (Mic Input, Stream Output)
  contextArray = filterOutgoingContexts(contextArray);

  // Filtreden sonra boÅŸ kaldÄ±ysa "No outgoing audio" gÃ¶ster
  if (contextArray.length === 0) {
    container.innerHTML = '<div class="no-data">No outgoing audio</div>';
    timestamp.textContent = '';
    return;
  }

  // Limit to MAX_AUDIO_CONTEXTS
  if (contextArray.length > MAX_AUDIO_CONTEXTS) {
    contextArray = contextArray.slice(0, MAX_AUDIO_CONTEXTS);
  }

  // Header timestamp = Context creation time (like other sections)
  const firstCtx = contextArray[0];
  timestamp.textContent = formatTime(firstCtx.static?.timestamp);

  let html = '';

  // â•â•â• DEBUG: Context Rendering â•â•â•
  console.log(`[AudioInspector] ðŸ” renderACStats: rendering ${contextArray.length} context(s)`);
  contextArray.forEach((ctx, index) => {
    console.log(`[AudioInspector] ðŸ” Context[${index}]:`, {
      contextId: ctx.contextId,
      inputSource: ctx.pipeline?.inputSource,
      hasMediaStreamSource: ctx.pipeline?.processors?.some(p => p.type === 'mediaStreamSource'),
      processorCount: ctx.pipeline?.processors?.length,
      state: ctx.static?.state
    });

    const purpose = getContextPurpose(ctx);
    console.log(`[AudioInspector] ðŸ” Context[${index}] purpose: ${purpose.label} (${purpose.icon})`);

    // Page Audio context â†’ minimal gÃ¶rÃ¼nÃ¼m (mikrofon yok mesajÄ± ile)
    if (purpose.label === 'Page Audio') {
      html += `<div class="context-item context-minimal${index > 0 ? ' context-separator' : ''}">
        <span class="context-purpose">${purpose.icon} ${purpose.label}</span>
        <span class="has-tooltip has-tooltip--info" data-tooltip="${purpose.tooltip}">â“˜</span>
        <span class="context-subtext">Site audio processing (VU meter, effects, etc.)</span>
      </div>`;
      return; // Skip detailed rendering
    }

    // Latency calculation: baseLatency (processing) + outputLatency (buffer/hardware)
    // baseLatency: inherent latency of AudioContext processing (~10ms typically)
    // outputLatency: additional output buffer latency (~32-42ms typically)
    const baseLatency = ctx.static?.baseLatency || 0;
    const outputLatency = ctx.static?.outputLatency || 0;
    const totalLatency = baseLatency + outputLatency;
    const latencyMs = totalLatency > 0 ? `${(totalLatency * 1000).toFixed(1)}ms` : '-';
    const stateClass = ctx.static?.state === 'running' ? 'good' : (ctx.static?.state === 'suspended' ? 'warning' : '');

    // â”â”â” Processor bilgilerini hazÄ±rla â”â”â”
    const ctxConnections = filterConnectionsByContext(audioConnections?.connections, [ctx]);
    console.log(`[AudioInspector] ðŸ” Audio Path: ctxConnections.length=${ctxConnections.length}`);
    const mainFromGraph = ctxConnections.length > 0
      ? deriveMainChainProcessorsFromConnections(ctxConnections, ctx)
      : [];
    console.log(`[AudioInspector] ðŸ” Audio Path: mainFromGraph.length=${mainFromGraph.length}`);
    let mainProcessors = mainFromGraph.length > 0
      ? mainFromGraph
      : (ctx.pipeline?.processors?.filter(p => p.type !== 'analyser') || []);
    console.log(`[AudioInspector] ðŸ” Audio Path: mainProcessors (before fallback)=`, mainProcessors);

    const hasInputSource = !!ctx.pipeline?.inputSource;

    // FALLBACK: If inputSource exists, ensure MediaStreamSource is in the chain
    if (hasInputSource && !mainProcessors.some(p => p.type === 'mediaStreamSource')) {
      mainProcessors = [
        { type: 'mediaStreamSource', timestamp: ctx.pipeline?.timestamp },
        ...mainProcessors
      ];
      console.log(`[AudioInspector] ðŸ” Audio Path: FALLBACK applied! Added mediaStreamSource to chain`);
    }

    // UI cleanliness: Show a single MediaStreamSource even if multiple captures were synced
    mainProcessors = dedupeMediaStreamSources(mainProcessors);
    console.log(`[AudioInspector] ðŸ” Audio Path: FINAL mainProcessors.length=${mainProcessors.length}`);

    const monitors = ctx.pipeline?.processors?.filter(p => p.type === 'analyser') || [];

    // Processing ve Effects bilgilerini Ã§Ä±kar
    const { processingText, effectsText } = extractProcessingInfo(mainProcessors, monitors);

    // Context item baÅŸlat (purpose baÅŸlÄ±ÄŸÄ± YOK - doÄŸrudan Context Info)
    html += `<div class="context-item${index > 0 ? ' context-separator' : ''}">`;

    // â”â”â” Context Info â”â”â” (Input dahil - tÃ¼m temel bilgiler)
    const channelTooltip = 'Output channel capacity (destination.maxChannelCount)';
    const latencyTooltip = `Total output latency (baseLatency: ${(baseLatency * 1000).toFixed(1)}ms + outputLatency: ${(outputLatency * 1000).toFixed(1)}ms). Input latency is shown in getUserMedia section.`;

    // Input label (ikon ile)
    const inputLabel = hasInputSource
      ? `${purpose.icon} ${ctx.pipeline.inputSource.charAt(0).toUpperCase() + ctx.pipeline.inputSource.slice(1)}`
      : '-';

    html += `
      <div class="ac-section ac-section--first">
        <div class="sub-header sub-header--ac">
          <span class="ac-section-title">Context Info</span>
        </div>
        <table class="ac-main-table">
          <tbody>
            <tr><td>Input</td><td>${inputLabel}</td></tr>
            <tr><td><span class="has-tooltip" data-tooltip="${channelTooltip}">Channels</span></td><td class="metric-value">${ctx.static?.channelCount || '-'}</td></tr>
            <tr><td>State</td><td class="${stateClass}">${ctx.static?.state || '-'}</td></tr>
            <tr><td><span class="has-tooltip" data-tooltip="${latencyTooltip}">Latency</span></td><td>${latencyMs}</td></tr>
            ${processingText ? `<tr><td>Processing</td><td>${processingText}</td></tr>` : ''}
            ${effectsText ? `<tr><td>Effects</td><td>${effectsText}</td></tr>` : ''}
          </tbody>
        </table>
      </div>
    `;

    // â”â”â” Audio Path â”â”â” (Tree yapÄ±sÄ±nda)
    const hasMainProcessors = mainProcessors.length > 0;

    if (hasInputSource || hasMainProcessors) {
      const pipelineTs = formatTime(ctx.pipeline?.timestamp);

      html += `
        <div class="ac-section">
          <div class="sub-header sub-header--ac">
            <span class="ac-section-title">Audio Path</span>
            <span class="ac-detected-time-subtle">(${pipelineTs})</span>
          </div>
          ${renderAudioPathTree(mainProcessors, monitors, ctx.pipeline?.inputSource)}
        </div>
      `;
    }

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // PCM PROCESSING HINT: Show raw audio processing (e.g., ScriptProcessor)
    // Indicates raw PCM data is being processed (not yet encoded)
    // Encoding detection is handled separately in Encoding section
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if (ctx.encodingHint) {
      html += `
        <div class="ac-section pcm-processing-section">
          <div class="processing-item">
            <span class="detail-label">âš ï¸ Processing</span>
            <span class="detail-value has-tooltip" data-tooltip="Raw audio data - not yet encoded">${escapeHtml(ctx.encodingHint.hint)}</span>
          </div>
        </div>`;
    }

    html += `</div>`;
  });

  console.log(`[AudioInspector] ðŸ” renderACStats: HTML length=${html.length}, container=`, container);
  console.log(`[AudioInspector] ðŸ” renderACStats: HTML preview (first 500 chars):`, html.substring(0, 500));
  container.innerHTML = html;
  console.log(`[AudioInspector] ðŸ” renderACStats: DOM updated, container.children.length=`, container.children.length);
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// ENCODER DETECTORS - OCP Compliant Encoder Detection Pattern
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
//
// Purpose: Detect and extract encoding information from different sources
// Pattern: Array-based detector pattern for Open-Closed Principle compliance
//
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// PRIORITY ORDER (CRITICAL - DO NOT REORDER WITHOUT UNDERSTANDING IMPACT)
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// Array.find() returns the FIRST matching detector, so order determines priority:
//
// 1. Detected Encoder        â†’ Highest priority (explicit, most reliable)
//    - Direct encoder detection via WASM (libopus/libmp3lame), Blob analysis, etc.
//    - Provides: codec, bitrate, sample rate, channels
//    - Reliability: â˜…â˜…â˜…â˜…â˜… (Kesin tespit)
//
// 2. WebRTC (RTCPeerConnection) â†’ High priority (explicit from stats)
//    - RTCPeerConnection.getStats() outbound-rtp codec
//    - Provides: codec, bitrate, packetization
//    - Reliability: â˜…â˜…â˜…â˜…â˜† (Stats API'den kesin)
//
// 3. PendingMediaRecorder    â†’ MediaRecorder active but mimeType empty
//    - Catches edge case where browser hasn't set mimeType yet
//    - Shows pulse animation until mimeType becomes available
//    - Reliability: â˜…â˜…â˜…â˜†â˜† (API var ama codec henÃ¼z bilinmiyor)
//
// 4. MediaRecorder           â†’ Medium-high priority (explicit from API)
//    - MediaRecorder.mimeType detection
//    - Provides: codec, bitrate (if specified)
//    - Reliability: â˜…â˜…â˜…â˜…â˜† (API'den kesin)
//
// 5. PendingWebAudio         â†’ WebAudio pipeline detected, encoder pending
//    - AudioWorklet/ScriptProcessor with mic/system input
//    - Shows pulse animation until Blob is created
//    - Reliability: â˜…â˜…â˜…â˜†â˜† (Pipeline var, encoder henÃ¼z bilinmiyor)
//
// 6. ScriptProcessor         â†’ LOWEST priority (heuristic guess)
//    - ScriptProcessor presence in AudioContext pipeline
//    - Provides: educated guess (may encode to WAV/MP3)
//    - Reliability: â˜…â˜…â˜†â˜†â˜† (Tahmin - guarantee yok)
//    - Only shown when NO explicit encoder detected
//
// âš ï¸ WARNING: Changing array order will break priority logic!
// âš ï¸ ScriptProcessor MUST be last - it's a fallback heuristic
//
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// HOW TO ADD NEW DETECTORS
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// 1. Determine reliability level:
//    - Explicit/API-based detection â†’ Add BEFORE ScriptProcessor
//    - Heuristic/guess-based â†’ Add AFTER existing detectors
//
// 2. Add detector object with:
//    { name, detect(data), extract(data) }
//
// 3. Insert at correct priority position (NOT at end if explicit!)
//
// 4. Update this comment block to document the new detector
//
// Example:
//   {
//     name: 'WebCodecs',
//     detect: (data) => !!data.webCodecs,
//     extract: (data) => ({ codec, bitrateKbps, source, rows })
//   }
//   â†’ Insert AFTER MediaRecorder (explicit API detection)
//
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// Pattern to user-friendly confidence label mapping
// Shows detection method (Worker/AudioWorklet) and data quality (full/basic)
//
// âš ï¸ SYNC REQUIRED: When adding new patterns in EarlyHook.js,
// add corresponding entry here. Pattern values must match exactly.
// Source: src/core/utils/EarlyHook.js â†’ encoderInfo.pattern assignments
// Source: scripts/early-inject.js â†’ Blob hook pattern assignments
const DETECTION_LABELS = {
  // AudioWorklet patterns (AudioWorklet.port.postMessage hook)
  'audioworklet-config': { text: 'AudioWorklet (full)', icon: 'âœ“', tooltip: 'AudioWorklet.port.postMessage hook - all encoder parameters captured' },
  'audioworklet-init': { text: 'AudioWorklet (basic)', icon: 'â—‹', tooltip: 'AudioWorklet.port.postMessage hook - only basic parameters' },
  'audioworklet-deferred': { text: 'AudioWorklet (late)', icon: 'â—', tooltip: 'AudioWorklet detected after context registration' },
  // Worker patterns (Worker.postMessage hook)
  'direct': { text: 'Worker Hook (full)', icon: 'âœ“', tooltip: 'Worker.postMessage hook - full encoder configuration' },
  'nested': { text: 'Worker Hook (full)', icon: 'âœ“', tooltip: 'Worker.postMessage hook - config in nested message' },
  'worker-init': { text: 'Worker Hook (basic)', icon: 'â—‹', tooltip: 'Worker.postMessage hook - basic initialization' },
  'worker-audio-init': { text: 'Worker (real-time)', icon: 'â—', tooltip: 'Audio worker init detected - codec confirmed, bitrate may be default' },
  // Blob creation patterns (audio file created - post-hoc detection)
  'audio-blob': { text: 'Blob (post-hoc)', icon: 'â—‘', tooltip: 'Audio file detected via Blob creation - encoding format confirmed after recording stopped' },
  // Default
  'unknown': { text: 'Detected', icon: '?', tooltip: 'Encoder detected but method unknown' }
};

const ENCODER_DETECTORS = [
  {
    name: 'DetectedEncoder',
    detect: (data) => {
      // Skip if no encoder data detected
      if (!data.detectedEncoder) return false;

      // If MediaRecorder is ACTIVE and encoder is from Blob detection only,
      // defer to MediaRecorder detector only when blob format matches MediaRecorder output.
      // Otherwise keep Blob signal (e.g., PCM/WAV export while a MediaRecorder exists on the page).
      const isOnlyBlobDetection = data.detectedEncoder.pattern === 'audio-blob';
      const mr = data.mediaRecorder;
      const mrIsActive = mr?.state === 'recording' || mr?.state === 'paused';
      const hasActiveMediaRecorder = mrIsActive && !!mr?.mimeType && mr?.hasAudioTrack !== false;
      if (isOnlyBlobDetection && hasActiveMediaRecorder) {
        const normalizeMime = (m) => (typeof m === 'string' ? m.split(';')[0].trim().toLowerCase() : '');
        const blobMimeBase = normalizeMime(data.detectedEncoder.mimeType);
        const mrMimeBase = normalizeMime(mr.mimeType);
        const sameBaseMime = blobMimeBase && mrMimeBase && blobMimeBase === mrMimeBase;

        const blobCodec = String(data.detectedEncoder.codec || '').toLowerCase();
        const blobContainer = String(data.detectedEncoder.container || '').toLowerCase();
        const isWavLike = blobCodec === 'pcm' || blobContainer === 'wav' || blobMimeBase === 'audio/wav' || blobMimeBase === 'audio/wave';

        if (!isWavLike && sameBaseMime) {
          return false; // Let MediaRecorder detector handle native recorder output
        }
      }

      return true;
    },
    extract: (data) => {
      const enc = data.detectedEncoder;

      // Build codec display with application type suffix if available
      // e.g., "OPUS (VoIP)", "OPUS (Audio)", "OPUS (LowDelay)"
      const rawCodec = enc.codec ?? 'unknown';
      const isUnknownCodec = typeof rawCodec === 'string' && rawCodec.toLowerCase() === 'unknown';

      const normalizeMime = (m) => (typeof m === 'string' ? m.split(';')[0].trim().toLowerCase() : '');
      const mimeBase = normalizeMime(enc.mimeType);
      const codecLower = String(rawCodec || '').toLowerCase();
      const containerLower = String(enc.container || '').toLowerCase();
      const isLinearPcmWav = codecLower === 'pcm' && (
        containerLower === 'wav' ||
        mimeBase === 'audio/wav' ||
        mimeBase === 'audio/wave'
      );

      // Show "Detecting..." for unknown codec (will be confirmed when Blob is created)
      // Codec = format (PCM, OPUS, MP3), Encoder = process (Linear PCM, Opus WASM)
      const codecBase = isUnknownCodec
        ? renderStatusPulse('Detecting...', 'Codec will be confirmed when recording stops and audio file is created')
        : (isLinearPcmWav ? 'PCM' : String(rawCodec).toUpperCase());
      const codecDisplay = enc.applicationName
        ? `${codecBase} (${enc.applicationName})`
        : codecBase;

      // Build rows dynamically based on available data
      const rows = [
        { label: 'Codec', value: codecDisplay, isMetric: true }
      ];

      // Encoder info (opus-wasm, mp3-wasm, aac-wasm, vorbis-wasm, flac-wasm, pcm)
      // Detection info shown only in tooltip
      if (enc.encoder) {
        const encoderDisplay = formatEncoderDisplay(enc.encoder);
        const detection = DETECTION_LABELS[enc.pattern] || DETECTION_LABELS['unknown'];

        // Build tooltip: detection first, then library/worker/path details
        const tooltipParts = [`detection: ${detection.text}`];
        if (enc.library) tooltipParts.push(`Library: ${enc.library}`);
        if (enc.workerFilename) tooltipParts.push(`Worker: ${enc.workerFilename}`);
        if (enc.encoderPath) tooltipParts.push(`Path: ${String(enc.encoderPath).split('/').pop()}`);
        if (enc.processorName) tooltipParts.push(`Worklet: ${enc.processorName}`);
        const encoderTooltip = tooltipParts.join(' | ');

        rows.push({
          label: 'Encoder',
          value: `<span class="has-tooltip" data-tooltip="${escapeHtml(encoderTooltip)}">${encoderDisplay}</span>`,
          isMetric: true
        });
      } else {
        rows.push({ label: 'Encoder', value: '-', isMetric: false });
      }

      // Container format (OGG, WebM, WAV, MP4, etc.)
      // Always show - use "-" if not detected
      if (enc.container) {
        rows.push({ label: 'Container', value: enc.container.toUpperCase(), isMetric: true });
      } else {
        rows.push({ label: 'Container', value: '-', isMetric: false });
      }

      // Library (underlying C library: libopus, LAME, FDK AAC, etc.)
      // Always show - use "-" if not available (e.g., PCM has no library)
      if (enc.library) {
        rows.push({ label: 'Library', value: enc.library, isMetric: true });
      } else {
        rows.push({ label: 'Library', value: '-', isMetric: false });
      }

      // Bit Depth (important for PCM/WAV - shows sample format: 16-bit int, 32-bit float, etc.)
      if (enc.wavBitDepth) {
        rows.push({ label: 'Bit Depth', value: `${enc.wavBitDepth}bit`, isMetric: true });
      } else {
        rows.push({ label: 'Bit Depth', value: '-', isMetric: false });
      }

      // Bitrate - always show, dynamically calculated from blob size / duration
      if (enc.bitRate && enc.bitRate > 0) {
        rows.push({ label: 'Bitrate', value: `${Math.round(enc.bitRate / 1000)} kbps`, isMetric: true });
      } else if (enc.isLiveEstimate === true) {
        rows.push({
          label: 'Bitrate',
          value: '<span class="has-tooltip" data-tooltip="Recording in progress - bitrate will be calculated when recording stops">Calculating...</span>',
          isMetric: false
        });
      } else {
        rows.push({ label: 'Bitrate', value: '-', isMetric: false });
      }

      // Frame size (if available) - smart unit detection for Opus
      if (enc.frameSize) {
        // Opus frame sizes: 2.5, 5, 10, 20, 40, 60 ms OR 120, 240, 480, 960, 1920, 2880 samples (48kHz)
        const msValues = [2.5, 5, 10, 20, 40, 60];
        const unit = msValues.includes(enc.frameSize) || enc.frameSize < 100 ? 'ms' : 'samples';
        rows.push({ label: 'Frame', value: `${enc.frameSize} ${unit}`, isMetric: false });
      }

      // Input: Detect technology from AudioContext pipeline
      // Shows WHAT TECHNOLOGY processes audio before encoding (Worklet > ScriptProcessor > WebAudio)
      const deriveEncoderInput = () => {
        const contexts = Array.isArray(data.audioContext)
          ? data.audioContext
          : (data.audioContext ? [data.audioContext] : []);

        for (const ctx of contexts) {
          const processors = ctx?.pipeline?.processors || [];
          if (processors.length === 0) continue;

          const technology = detectEncoderInputTechnology(processors);
          if (technology) return technology;
        }
        return null;
      };

      const encoderInput = deriveEncoderInput();
      rows.push({
        label: 'Input',
        value: encoderInput || '-',
        isMetric: !!encoderInput
      });

      return {
        codec: enc.codec ? String(enc.codec).toUpperCase() : 'UNKNOWN',
        bitrateKbps: enc.bitRate ? `${Math.round(enc.bitRate / 1000)}` : '-',
        source: enc.workerFilename || enc.processorName || 'Worker',
        timestamp: enc.timestamp || Date.now(),
        rows
      };
    }
  },
  {
    name: 'WebRTC',
    detect: (data) => data.rtcStats?.peerConnections?.length > 0,
    extract: (data) => {
      const pc = data.rtcStats.peerConnections.find(c => c.send) || data.rtcStats.peerConnections[0];
      if (!pc?.send?.codec) return null;

      const codecRaw = pc.send.codec.split('/')[1] || pc.send.codec;
      const codec = codecRaw.toUpperCase();
      const bitrateKbps = pc.send.bitrateKbps || '-';

      const rows = [
        { label: 'Codec', value: codec, isMetric: true },
        { label: 'Bitrate', value: `${bitrateKbps} kbps`, isMetric: true },
        // Encoder type - Browser's built-in WebRTC encoder
        {
          label: 'Encoder',
          value: '<span class="has-tooltip" data-tooltip="Browser\'s built-in WebRTC audio encoder">ðŸŒ WebRTC Native</span>',
          isMetric: false
        }
      ];

      // Opus params (DTX, FEC, CBR/VBR, stereo) - if available
      if (pc.send.opusParams) {
        const op = pc.send.opusParams;
        const modeParts = [];
        if (op.cbr !== undefined) modeParts.push(op.cbr ? 'CBR' : 'VBR');
        if (op.dtx !== undefined) modeParts.push(`DTX:${op.dtx ? 'on' : 'off'}`);
        if (op.fec !== undefined) modeParts.push(`FEC:${op.fec ? 'on' : 'off'}`);
        if (op.stereo !== undefined) modeParts.push(op.stereo ? 'Stereo' : 'Mono');
        if (modeParts.length > 0) {
          rows.push({ label: 'Mode', value: modeParts.join(' / '), isMetric: false });
        }
      }

      return {
        codec,
        bitrateKbps,
        source: 'WebRTC',
        timestamp: data.rtcStats.timestamp || Date.now(),
        rows
      };
    }
  },
  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  // PendingMediaRecorder Detector
  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  // Detects MediaRecorder that is actively recording but mimeType is not yet available.
  // Some browsers/sites don't set mimeType until start() or first dataavailable event.
  // This ensures we show "recording in progress" instead of "no encoder".
  // Priority: BEFORE MediaRecorder (catches the "mimeType empty" edge case)
  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  {
    name: 'PendingMediaRecorder',
    detect: (data) => {
      const mr = data.mediaRecorder;
      if (!mr) return false;
      const isActive = mr.state === 'recording' || mr.state === 'paused';
      const hasMimeType = !!mr.mimeType;
      // Active MediaRecorder without mimeType
      return isActive && !hasMimeType;
    },
    extract: (data) => {
      const mr = data.mediaRecorder;
      const inputInfo = getInputSourceInfo(mr.audioSource, mr.hasAudioTrack);

      const rows = [
        {
          label: 'Codec',
          value: renderStatusPulse('Detecting...', 'Codec will be determined when first audio data is available'),
          isMetric: false
        },
        {
          label: 'Container',
          value: renderStatusPulse('Detecting...', 'Container format will be determined from mimeType'),
          isMetric: false
        },
        {
          label: 'Bitrate',
          value: mr.audioBitsPerSecond
            ? `${Math.round(mr.audioBitsPerSecond / 1000)} kbps`
            : renderStatusPulse('Detecting...', 'Bitrate will be available when encoding starts'),
          isMetric: !!mr.audioBitsPerSecond
        },
        {
          label: 'Encoder',
          value: '<span class="has-tooltip" data-tooltip="Browser\'s built-in MediaRecorder API">ðŸŒ MediaRecorder API</span>',
          isMetric: true
        },
        {
          label: 'State',
          value: `<span class="badge badge-code">${mr.state}</span>`,
          isMetric: false
        }
      ];

      // Add input source if available
      const inputRow = buildInputRow(inputInfo);
      if (inputRow) rows.push(inputRow);

      return {
        codec: 'Detecting...',
        bitrateKbps: '-',
        source: 'MediaRecorder',
        timestamp: mr.timestamp || Date.now(),
        rows
      };
    }
  },
  {
    name: 'MediaRecorder',
    detect: (data) => !!data.mediaRecorder?.mimeType,
    extract: (data) => {
      const mr = data.mediaRecorder;
      const mimeTypeLower = (mr.mimeType || '').toLowerCase();

      // Extract codec from various sources
      let codecRaw = mr.parsedMimeType?.codec ||
        mr.mimeType.split('codecs=')[1]?.replace(/['"]/g, '') || '';

      // Edge Case: MP3 detection from mimeType when codecs= parameter is absent
      // MP3 is self-contained (codec IS the format), so mimeType alone is sufficient
      if (!codecRaw && (mimeTypeLower.includes('audio/mp3') || mimeTypeLower.includes('audio/mpeg'))) {
        codecRaw = 'mp3';
      }

      const codec = codecRaw ? codecRaw.toUpperCase() : '-';
      const container = mr.parsedMimeType?.container?.toUpperCase() || '';
      const bitrateKbps = mr.audioBitsPerSecond
        ? `${Math.round(mr.audioBitsPerSecond / 1000)}`
        : '-';

      const stateClass = mr.state === 'recording' ? 'good' :
        (mr.state === 'paused' ? 'warning' : '');

      const rows = [
        { label: 'Codec', value: codec, isMetric: true }
      ];

      if (container) {
        rows.push({ label: 'Container', value: container, isMetric: false });
      }

      rows.push({ label: 'Bitrate', value: `${bitrateKbps} kbps`, isMetric: true });

      // Encoder type - Browser's built-in MediaRecorder API (not WASM)
      // This distinguishes from WASM encoders like opus-recorder, lamejs, etc.
      rows.push({
        label: 'Encoder',
        value: '<span class="has-tooltip" data-tooltip="Browser\'s built-in MediaRecorder API - not a JavaScript/WASM library">ðŸŒ MediaRecorder API</span>',
        isMetric: false
      });

      if (mr.state) {
        rows.push({
          label: 'State',
          value: `<span class="badge badge-code">${mr.state}</span>`,
          isMetric: false,
          cssClass: stateClass
        });
      }

      // Input source - shows where audio comes from (technical but useful)
      // 'synthesized' = AudioContext pipeline (PCM processed via ScriptProcessor/AudioWorklet)
      // 'microphone' = Direct microphone input
      // 'system' = System audio capture
      console.log('[Popup] MediaRecorder Input Debug:', {
        audioSource: mr.audioSource,
        hasAudioTrack: mr.hasAudioTrack,
        trackInfo: mr.trackInfo
      });
      const inputInfo = getInputSourceInfo(mr.audioSource, mr.hasAudioTrack);
      console.log('[Popup] getInputSourceInfo result:', inputInfo);
      const inputRow = buildInputRow(inputInfo);
      if (inputRow) {
        rows.push(inputRow);
      } else {
        console.log('[Popup] âš ï¸ inputInfo is null - Input row NOT added');
      }

      return {
        codec,
        bitrateKbps,
        source: 'MediaRecorder',
        timestamp: mr.timestamp || Date.now(),
        rows
      };
    }
  },
  {
    name: 'PendingWebAudio',
    detect: (data) => {
      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      // SINGLE SOURCE OF TRUTH: Use recordingActive from early-inject.js
      // This is the ONLY reliable indicator - no complex heuristics needed
      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      if (data.detectedEncoder) return false;
      if (data.rtcStats?.peerConnections?.length > 0) return false;
      if (data.mediaRecorder?.mimeType) return false;
      if (!data.audioContext) return false;

      // Check for active audio pipeline with microphone input
      const contexts = Array.isArray(data.audioContext) ? data.audioContext : [data.audioContext];
      const hasActiveAudioPipeline = contexts.some(ctx => {
        const input = ctx?.pipeline?.inputSource;
        const processors = ctx?.pipeline?.processors || [];
        const hasProcessing = processors.some(p => p.type === 'audioWorkletNode' || p.type === 'scriptProcessor');
        return (input === 'microphone' || input === 'system' || input === 'synthesized') && hasProcessing;
      });

      if (!hasActiveAudioPipeline) return false;

      // SIMPLE: Just check if recording is active (set by MediaRecorder.start or Blob tracking)
      return data.recordingActive?.active === true;
    },
    extract: (data) => {
      const contexts = Array.isArray(data.audioContext) ? data.audioContext : [data.audioContext];
      const ctx = contexts.find(c => {
        const input = c?.pipeline?.inputSource;
        const processors = c?.pipeline?.processors || [];
        const hasProcessing = processors.some(p => p.type === 'audioWorkletNode' || p.type === 'scriptProcessor');
        return (input === 'microphone' || input === 'system' || input === 'synthesized') && hasProcessing;
      });
      if (!ctx) return null;

      const processors = ctx.pipeline?.processors || [];
      const pipelineType = processors.some(p => p.type === 'audioWorkletNode')
        ? 'AudioWorklet'
        : (processors.some(p => p.type === 'scriptProcessor') ? 'ScriptProcessor' : 'WebAudio');

      // Derive Input from AudioContext pipeline (this CAN be detected during recording)
      const encoderInput = detectEncoderInputTechnology(processors);

      return {
        codec: 'Detecting...',
        bitrateKbps: '-',
        source: 'WebAudio',
        timestamp: ctx.pipeline?.timestamp || ctx.static?.timestamp || Date.now(),
        rows: [
          {
            label: 'Codec',
            value: renderStatusPulse('Detecting...', 'Codec will be confirmed when the final audio Blob is created.'),
            isMetric: false
          },
          {
            label: 'Encoder',
            value: renderStatusPulse('Detecting...', `WebAudio pipeline detected (${pipelineType}). Encoder will be confirmed when encoding starts.`),
            isMetric: false
          },
          {
            label: 'Container',
            value: renderStatusPulse('Detecting...', 'Container format will be determined from the output Blob mimeType.'),
            isMetric: false
          },
          {
            label: 'Library',
            value: renderStatusPulse('Detecting...', 'Underlying library (libopus, LAME, etc.) will be detected from WASM/Worker analysis.'),
            isMetric: false
          },
          {
            label: 'Bit Depth',
            value: renderStatusPulse('Detecting...', 'Bit depth will be extracted from WAV header or encoder config.'),
            isMetric: false
          },
          {
            label: 'Bitrate',
            value: renderStatusPulse('Calculating...', 'Bitrate will be calculated from Blob size and duration when recording stops.'),
            isMetric: false
          },
          {
            label: 'Input',
            value: `<span class="has-tooltip" data-tooltip="Audio processing technology detected in WebAudio pipeline">${encoderInput || pipelineType}</span>`,
            isMetric: true
          },
          {
            label: 'Confidence',
            value: renderStatusPulse('Pending...', 'Confidence will be determined based on detection evidence.'),
            isMetric: false
          }
        ]
      };
    }
  },

  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  // ScriptProcessor Detector (Heuristic - LOWEST PRIORITY)
  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  // Detects ScriptProcessor usage which MAY indicate encoding to WAV/MP3
  // This is a fallback when no explicit encoder (WASM/WebRTC/MediaRecorder) is found
  // Priority: Lowest (only shown when no other encoder is detected)
  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  {
    name: 'ScriptProcessor',
    detect: (data) => {
      // Check if any AudioContext has ScriptProcessor in pipeline
      if (!data.audioContext) return false;

      const contexts = Array.isArray(data.audioContext) ? data.audioContext : [data.audioContext];
      return contexts.some(ctx =>
        ctx.pipeline?.processors?.some(p => p.type === 'scriptProcessor')
      );
    },
    extract: (data) => {
      const contexts = Array.isArray(data.audioContext) ? data.audioContext : [data.audioContext];
      const ctx = contexts.find(c =>
        c.pipeline?.processors?.some(p => p.type === 'scriptProcessor')
      );

      if (!ctx) return null;

      const sp = ctx.pipeline.processors.find(p => p.type === 'scriptProcessor');
      const bufferSize = sp?.bufferSize || '-';
      const channels = sp?.inputChannels || sp?.outputChannels || '-';

      // ScriptProcessor works with raw PCM data (uncompressed audio samples)
      // It's not a codec - it processes raw Float32Array audio buffers
      // The actual encoding (if any) happens downstream (WAV/MP3 encoding in JS or Worker)
      // NOTE: Buffer/Channels NOT shown here - already displayed in AudioContext section (DRY)
      return {
        codec: 'Raw PCM',
        bitrateKbps: '-',
        source: 'ScriptProcessor',
        timestamp: ctx.static?.timestamp || Date.now(),
        rows: [
          {
            label: 'Format',
            value: '<span class="has-tooltip" data-tooltip="ScriptProcessor processes raw PCM audio samples (Float32Array). Actual encoding may happen downstream.">Raw PCM</span>',
            isMetric: true
          },
          {
            label: 'Confidence',
            value: '<span class="has-tooltip" data-tooltip="ScriptProcessor detected - may be encoding to WAV/MP3 downstream. This is a heuristic, not confirmed.">â—‹ Heuristic</span>',
            isMetric: false
          }
        ]
      };
    }
  }
];

// Render Encoding section - OCP compliant with detector pattern
function renderEncodingSection(detectedEncoder, rtcStats, mediaRecorder, audioContext, userMedia, recordingActive) {
  const container = document.getElementById('encodingContent');
  const timestamp = document.getElementById('encodingTimestamp');
  if (!container) return;

  const data = { detectedEncoder, rtcStats, mediaRecorder, audioContext, userMedia, recordingActive };

  // DEBUG: Log all available data
  console.log('[Popup] renderEncodingSection data:', {
    hasDetectedEncoder: !!detectedEncoder,
    detectedEncoderPattern: detectedEncoder?.pattern,
    hasRtcStats: !!rtcStats,
    hasMediaRecorder: !!mediaRecorder,
    mediaRecorderState: mediaRecorder?.state,
    mediaRecorderAudioSource: mediaRecorder?.audioSource,
    hasAudioContext: !!audioContext,
    hasUserMedia: !!userMedia,
    userMediaTimestamp: userMedia?.timestamp,
    recordingActive: recordingActive?.active
  });

  // Find first matching detector (priority order maintained by array order)
  const detector = ENCODER_DETECTORS.find(d => d.detect(data));
  console.log('[Popup] Selected detector:', detector?.name || 'NONE');

  if (!detector) {
    // No encoder detected
    container.innerHTML = '<div class="no-data">No encoder</div>';
    if (timestamp) timestamp.textContent = '';
    return;
  }

  const encoderData = detector.extract(data);

  if (!encoderData) {
    // Detected but extraction failed
    container.innerHTML = '<div class="no-data">No encoder</div>';
    if (timestamp) timestamp.textContent = '';
    return;
  }

  // Build HTML from rows
  let html = `<table><tbody>`;
  encoderData.rows.forEach(row => {
    const valueClass = row.isMetric ? 'class="metric-value"' : (row.cssClass ? `class="${row.cssClass}"` : '');
    html += `<tr><td>${row.label}</td><td ${valueClass}>${row.value}</td></tr>`;
  });
  html += `</tbody></table>`;

  container.innerHTML = html;
  if (timestamp) timestamp.textContent = formatTime(encoderData.timestamp);
}

/**
 * Get input source display info for MediaRecorder
 * Shows where the audio stream originates from
 * @param {string} audioSource - 'microphone', 'system', 'synthesized', 'remote', 'unknown', 'none'
 * @param {boolean} hasAudioTrack - Whether the stream has an audio track
 * @returns {{icon: string, label: string, tooltip?: string}|null}
 */
function getInputSourceInfo(audioSource, hasAudioTrack) {
  if (!hasAudioTrack) {
    return null; // Don't show input for video-only recordings
  }

  switch (audioSource) {
    case 'microphone':
      return {
        icon: 'ðŸŽ¤',
        label: 'Microphone',
        tooltip: 'Direct microphone input (getUserMedia)'
      };
    case 'system':
      return {
        icon: 'ðŸ”Š',
        label: 'System Audio',
        tooltip: 'System audio capture (loopback/stereo mix)'
      };
    case 'synthesized':
      return {
        icon: 'ðŸ”„',
        label: 'Web Audio',
        tooltip: 'Audio routed through Web Audio API (createMediaStreamDestination). PCM data may be processed via ScriptProcessor or AudioWorklet before encoding.'
      };
    case 'remote':
      return {
        icon: 'ðŸ“¡',
        label: 'Remote',
        tooltip: 'Remote audio stream routed into the page (e.g., WebRTC remote track)'
      };
    case 'unknown':
      return {
        icon: 'â“',
        label: 'Unknown',
        tooltip: 'Audio source could not be determined from track label'
      };
    default:
      // 'none' or truly undefined - don't show
      return null;
  }
}

/**
 * Build Input row object for ENCODER_DETECTORS
 * Centralizes input row creation to avoid DRY violations
 * @param {ReturnType<typeof getInputSourceInfo>} inputInfo - Result from getInputSourceInfo
 * @returns {{label: string, value: string, isMetric: boolean}|null}
 */
function buildInputRow(inputInfo) {
  if (!inputInfo) return null;
  return {
    label: 'Input',
    value: inputInfo.tooltip
      ? `<span class="has-tooltip" data-tooltip="${inputInfo.tooltip}">${inputInfo.icon} ${inputInfo.label}</span>`
      : `${inputInfo.icon} ${inputInfo.label}`,
    isMetric: false
  };
}

// Determine log line color class based on message content
function getLogColorClass(message, level) {
  // Priority 1: Level-based errors/warnings from logger
  if (level === 'error') return 'error';
  if (level === 'warn') return 'warn';

  const msgLower = message.toLowerCase();

  // Priority 2: Explicit errors (highest priority after level)
  if (msgLower.includes('error') ||
      msgLower.includes('failed') ||
      msgLower.includes('âŒ')) {
    return 'error';
  }

  // Priority 3: Success states (green) - completed actions
  if (msgLower.includes('âœ…') ||
      msgLower.includes('started') ||    // "Started" = completed start
      msgLower.includes('ready') ||
      msgLower.includes('success') ||
      msgLower.includes('loaded')) {
    return 'success';
  }

  // Priority 4: Info states (blue) - ongoing/initialization
  // Use broader match to catch all variants
  if (msgLower.includes('initializ') ||  // initialize, initialized, initializing, initialization
      msgLower.includes('starting')) {   // "Starting..." = ongoing
    return 'info';
  }

  // Priority 5: Warning states (orange)
  if (msgLower.includes('waiting') ||
      msgLower.includes('warning') ||
      msgLower.includes('âš ï¸')) {
    return 'warn';
  }

  // Default: no special class (uses default colors)
  return '';
}

// Render Debug Logs (compact single-line format)
function renderDebugLogs(logs) {
  const container = document.getElementById('debugContent');
  const logCount = logs?.length || 0;

  // Update badge
  updateLogBadge(logCount);

  if (!logs || logs.length === 0) {
    container.innerHTML = '<div class="no-data">Waiting for events...</div>';
    return;
  }

  let html = '';
  logs.forEach(log => {
    const time = formatTime(log.timestamp);
    const colorClass = getLogColorClass(log.message, log.level);

    html += `<div class="log-line ${colorClass}">
      <span class="log-time">${time}</span>
      <span class="log-prefix">[${escapeHtml(log.prefix)}]</span>
      <span class="log-message">${escapeHtml(log.message)}</span>
    </div>`;
  });

  container.innerHTML = html;
  container.scrollTop = container.scrollHeight;
}



// Export function
function exportData() {
  if (!latestData) {
    alert('No data to export');
    return;
  }
  
  const json = JSON.stringify(latestData, null, 2);
  const blob = new Blob([json], { type: 'application/json' });
  const url = URL.createObjectURL(blob);
  
  const a = document.createElement('a');
  a.href = url;
  a.download = `audio-inspector-${Date.now()}.json`;
  a.click();
  
  URL.revokeObjectURL(url);
}

// Clear data
async function clearData() {
  // First stop inspector if running (prevents collector from refilling data)
  const result = await chrome.storage.local.get(['inspectorEnabled', 'lockedTab']);
  if (result.inspectorEnabled && result.lockedTab) {
    try {
      await chrome.tabs.sendMessage(result.lockedTab.id, {
        type: 'SET_ENABLED',
        enabled: false
      });
    } catch (e) {
      // Tab may be closed, continue with clear
    }
  }

  // Then clear inspector storage (centralized) - keep persistent keys like platformInfo
  await clearInspectorData({ includeAutoStopReason: true });
  location.reload();
}

// Copy logs to clipboard
async function copyLogs() {
  const result = await chrome.storage.local.get('debug_logs');
  const logs = result.debug_logs || [];

  if (logs.length === 0) {
    alert('No logs to copy');
    return;
  }

  // Format logs as plain text
  const text = logs.map(log => {
    const time = formatTime(log.timestamp);
    return `${time} [${log.prefix}] ${log.message}`;
  }).join('\n');

  try {
    await navigator.clipboard.writeText(text);
    // Show feedback
    const btn = document.getElementById('copyLogsBtn');
    const originalText = btn.textContent;
    btn.textContent = 'Copied!';
    btn.style.color = 'var(--accent-green)';
    setTimeout(() => {
      btn.textContent = originalText;
      btn.style.color = '';
    }, 1500);
  } catch (err) {
    console.error('Failed to copy logs:', err);
    alert('Failed to copy logs');
  }
}

// Clear logs only
async function clearLogs() {
  await clearLogsOnly();
  renderDebugLogs([]);
  updateLogBadge(0);
}

// Toggle console drawer
function toggleDrawer() {
  drawerOpen = !drawerOpen;
  const drawer = document.getElementById('drawerOverlay');
  drawer.classList.toggle('open', drawerOpen);
}

// ========== Refresh Modal Functions ==========

// Show refresh required modal
function showRefreshModal(activeTab) {
  const modal = document.getElementById('refreshModal');
  modal.classList.add('visible');
  // Store tab info for confirm handler
  modal.dataset.tabId = activeTab.id;
}

// Hide refresh modal
function hideRefreshModal() {
  const modal = document.getElementById('refreshModal');
  modal.classList.remove('visible');
  delete modal.dataset.tabId;
}

// Handle refresh and start
async function handleRefreshAndStart() {
  const modal = document.getElementById('refreshModal');
  const tabId = parseInt(modal.dataset.tabId, 10);

  if (!tabId) {
    debugLog('âŒ No tab ID found for refresh');
    hideRefreshModal();
    return;
  }

  debugLog(`ðŸ”„ Refreshing tab ${tabId} and setting pendingAutoStart`);

  // Clear previous session data first
  await clearInspectorData({ includeAutoStopReason: true });

  // Set pendingAutoStart flag - content.js will auto-start after reload
  await chrome.storage.local.set({ pendingAutoStart: tabId });

  // Hide modal
  hideRefreshModal();

  // Reload the tab
  chrome.tabs.reload(tabId);
}

// Update log badge count
function updateLogBadge(count) {
  const badge = document.getElementById('logBadge');
  if (!badge) return;

  badge.textContent = count;
  badge.classList.toggle('empty', count === 0);
}

// Event listeners
document.getElementById('toggleBtn').addEventListener('click', toggleInspector);
document.getElementById('exportBtn').addEventListener('click', exportData);
document.getElementById('clearBtn').addEventListener('click', clearData);
document.getElementById('copyLogsBtn').addEventListener('click', copyLogs);
document.getElementById('clearLogsBtn').addEventListener('click', clearLogs);
document.getElementById('drawerHandle').addEventListener('click', toggleDrawer);

// Refresh modal event listeners
document.getElementById('refreshModalCancel').addEventListener('click', hideRefreshModal);
document.getElementById('refreshModalConfirm').addEventListener('click', handleRefreshAndStart);

// Tab deÄŸiÅŸikliÄŸini dinle - banner'Ä± gÃ¼ncelle
// Side panel global, tab deÄŸiÅŸince yeniden yÃ¼klenmiyor
chrome.tabs.onActivated.addListener((activeInfo) => {
  // currentTabId'yi gÃ¼ncelle (tabs.onUpdated iÃ§in gerekli)
  currentTabId = activeInfo.tabId;
  checkTabLock();
});

// Tab URL deÄŸiÅŸikliÄŸini dinle - navigation sonrasÄ± controls durumunu gÃ¼ncelle
// Ã–rn: chrome://newtab â†’ web sitesi geÃ§iÅŸinde Start butonunu aktif et
chrome.tabs.onUpdated.addListener((tabId, changeInfo) => {
  // Sadece URL deÄŸiÅŸikliklerini izle ve sadece mevcut tab iÃ§in
  if (changeInfo.url && tabId === currentTabId) {
    checkTabLock();
  }
});

// Listen for storage changes instead of polling
chrome.storage.onChanged.addListener((changes, areaName) => {
  if (areaName === 'local') {
    // Only update UI if relevant keys changed (uses DATA_STORAGE_KEYS for DRY)
    // Note: audioWorklet data is now merged into audio_contexts, but keep key for direct updates
    const shouldUpdate = Object.keys(changes).some(key => DATA_STORAGE_KEYS.includes(key));

    if (shouldUpdate) {
      // Use debounced update to batch rapid storage changes
      // (e.g., audio_contexts + audio_connections written separately but close together)
      updateUIDebounced();
    }

    // Update logs if they changed
    if (changes.debug_logs) {
      renderDebugLogs(changes.debug_logs.newValue);
    }

    // Also check for inspectorEnabled change
    if (changes.inspectorEnabled) {
      const newValue = changes.inspectorEnabled.newValue === true;
      // Only update if actually changed (prevent race conditions)
      if (enabled !== newValue) {
        enabled = newValue;
        updateToggleButton();
      }
      // Tab switch'te banner'Ä± da gÃ¼ncelle (inspectorEnabled silindiÄŸinde)
      checkTabLock();
    }

    // lockedTab deÄŸiÅŸikliÄŸi (tab kapatÄ±ldÄ±ÄŸÄ±nda background.js tarafÄ±ndan silinir)
    if (changes.lockedTab) {
      // lockedTab silindiyse enabled'Ä± da sÄ±fÄ±rla (inspectorEnabled aynÄ± anda silinmiÅŸ olabilir)
      if (!changes.lockedTab.newValue) {
        enabled = false;
        updateToggleButton();
      }
      // lockedTab silindiyse veya deÄŸiÅŸtiyse tab lock kontrolÃ¼ yap
      checkTabLock();
    }
  }
});

// Panel kapanma bildirimi iÃ§in port-based connection (beforeunload gÃ¼venilir deÄŸil)
// Port aÃ§Ä±ldÄ±ÄŸÄ±nda background.js'e tabId gÃ¶nderilir, panel kapandÄ±ÄŸÄ±nda port otomatik disconnect olur
let panelPort = null;

function setupPanelPort() {
  if (!currentTabId) return;

  panelPort = chrome.runtime.connect({ name: `sidepanel-${currentTabId}` });
  panelPort.onDisconnect.addListener(() => {
    // Port kapandÄ± - bu normal, panel kapanÄ±yor demek
  });
}

// Initial load
loadEnabledState().then(async () => {
  // Panel aÃ§Ä±ldÄ±ÄŸÄ±nda aktif tab ID'sini kaydet (kapanma bildirim iÃ§in)
  const [activeTab] = await chrome.tabs.query({ active: true, currentWindow: true });
  currentTabId = activeTab?.id;

  // Port-based connection kur (panel kapanÄ±nca background.js otomatik bilgilendirilir)
  setupPanelPort();

  // Tab kilitleme kontrolÃ¼
  await checkTabLock();

  // If inspector is not enabled AND no locked tab exists, clear any old data
  // (lockedTab varsa veriler o tab'a ait, review iÃ§in korunmalÄ±)
  const { lockedTab } = await chrome.storage.local.get(['lockedTab']);
  if (!enabled && !lockedTab) {
    await clearSessionData();
  }
  updateUI();
});
